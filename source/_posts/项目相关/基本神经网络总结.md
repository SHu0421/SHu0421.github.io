---
title: 基本神经网络相关问题
categories:
  - 项目相关
date: 2021-11-13 20:11:27
tags: DNN
index_img: https://img.xjh.me/random_img.php?type=bg&ctype=nature&return=302
---
1. 滤波和传统的图像算法 e.g. SIFT

2. CNN感受野定义以及计算方式
感受野指的是一个特定的 CNN 特征（特征图上的某个点）在输入空间所受影响的区域。
感受野的计算是输出图像的反向过程
out_put=(iput_size-kernel_size)/stride+1
input_size=(out_put-1)*stride+kernel_size


3. CNN矩阵乘法原理
参考链接：[卷积计算](https://shuokay.com/2016/06/08/convolution/)
img2col将卷积运算转为矩阵乘法
其中输入的矩阵：行方向对应特征图不同channel同一视野（位置）的向量展开；列方向代表不同的位置；
kernel的矩阵：行方向代表代表不同的卷积核，列方向代表一个滤波器每个channel的矩阵->向量，
输出矩阵

4. BatchNormalization与正则化的区别：
    BatchNormalization主要是用于将神经网络中间输入特征图的均值和方差进行规范化，使输入分布拉到容易激活后面激活层中，解决梯度消失和梯度爆炸问题问题
    正则化，通常用于机器学习，在损失函数中加上参数的一阶或者二阶和，防止过拟合


5. 产生梯度消失或爆炸的原因：
网络层数太深而引发的梯度反向传播中的连乘效应
解决梯度消失或者梯度爆炸问题的解决方案：
   - batch BatchNormalization
   - 梯度裁剪
   - 修改激活函数，比如leaky relu
   - 残差结构
   - 每层预训练后微调



6. 机器学习：多分类如何转为多个二分类问题：需要训练N个分类器
[参考链接](https://yuanxiaosc.github.io/2018/07/01/%E4%BA%8C%E5%88%86%E7%B1%BB%E3%80%81%E5%A4%9A%E5%88%86%E7%B1%BB%E4%B8%8E%E5%A4%9A%E6%A0%87%E7%AD%BE%E9%97%AE%E9%A2%98%E7%9A%84%E5%8C%BA%E5%88%AB%E2%80%94%E2%80%94%E5%AF%B9%E5%BA%94%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E7%9A%84%E9%80%89%E6%8B%A9/)
将每一次的一个类作为正例，其余作为反例，总共训练N个分类器。测试的时候若仅有一个分类器预测为正的类别则对应的类别标记作为最终分类结果，若有多个分类器预测为正类，则选择置信度最大的类别作为最终分类结果。
PS:目标检测的多标签二分类损失函数是使用每个标签的概率的求二分损失函数（逻辑损失）然后求平均值得到整体的损失
其他方案：
一对一，建立N(N-1)/2个分类器，然后进行投票
直接用softmax进行多分类任务训练


7. 网络退化以及resnet为什么可以解决梯度消失和网络退化问题：
首先介绍什么是网络退化问题
举个例子，假设已经有了一个最优化的网络结构，是18层。当我们设计网络结构的时候，我们并不知道具体多少层次的网络时最优化的网络结构，假设设计了34层网络结构。那么多出来的16层其实是冗余的，我们希望训练网络的过程中，模型能够自己将这16层冗余层训练为恒等映射，也就是经过这层时的输入与输出完全一样。
但是往往模型很难将这16层恒等映射的参数学习正确，那么就不如最优化的18层网络结构的性能，这就是随着网络深度增加，模型会产生退化现象。它不是由过拟合产生的，而是由冗余的网络层学习了不是恒等映射的参数造成的。
也就是说神经网络学习恒等映射 F(x)=x 的效果不好，但是加上了skip connection 神经网络就用于学习F(x)=0

8. 链式法则：
原本y对x求偏导，但是由于过程较为复杂，我们引入了一个中间层z，先利用y对z求偏导，在乘上z对x求偏导，这样会使整个计算更为简单。
当前error对当前层参数的导数，是error对当前输出（输出特征图）的倒数*当前节点的输入（输入特征图）


9. 矩阵的反向传播算法/CNN的反向梯度传播算法



10. 激活函数sigmoid，tanh，relu. 各自的优点和适用场景
[参考链接]（https://zhuanlan.zhihu.com/p/71882757）
激活函数的作用：主要是向神经网络中引入非线性的特征
- sigmoid函数：主要运用在逻辑二分类中，缺点：计算复杂，不是0均值，同时容易出现梯度饱和，当输入是非常大的正值或者负值的时候，反向传播梯度为0；
- tanh函数：是0均值，但是计算复杂，同时也容易出现梯度饱和问题
- relu函数：计算简单，在输入大于0的时候不会出现梯度饱和，但是在输入小于0的时候会出现梯度为0，造成死的神经元，因此参数一直得不到更新


11. 常见神经网络的结构特点和结构优势

12. 牛顿法和SGD的各自原理，优势，区别

13. LR的梯度反向传播推导
https://blog.csdn.net/ft_sunshine/article/details/105370528

方程：y=theta*x
激活函数 h(x)-1/(1+e^(-theta x))
损失函数 loss=1/m*求和(y^ilog(h(x)) - (1-y^i)log(1-h(x))))

