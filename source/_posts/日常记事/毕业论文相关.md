---
title: 毕业论文相关
categories:
  - 日常记事
date: 2022-05-29 16:17:07
tags:
---


### 毕业论文思路
5-22 毕业论文计划Update
看了一下之前写的论文，目前的打算：
1. 毕业论文在之前写的算法优化相关的论文上+一些调度/异步重要性/缓存（字典）/分布式训练/pipeline 相关知识
从算法+系统上构建一个针对不同瓶颈场景的深度学习训练系统
算法上一些理论（使用历史值） 对系统上的优化提供了帮助
系统上可从：分布式计算通讯减少，缓存设计/减少冗余数据移动加速IO
  <font color=red>算法上优化目的减少计算时间，系统上优化目的减少通讯和IO时间</font>
  <font color=red> 重要性计算(只对重要的数据进行数据增强，有论文，或者对不重要的数据进行增强后缓存)， 重要性IO（高频更新和读取重要的样本，低频读取不重要样本，异步更新重要性）， 重要性通讯（梯度异步通讯，梯度小的就不同步，梯度比较大的同步，这样同步等待时间降低） </font>
  就相当于有三轮重要性过滤，可以通过参数指定是否要使用某种方法

  a. 把重要性采样算法：转为过滤和打分阶段
  b. 通讯：<font color=red>重要性采样算法情况下如何解决重要性采样算法带来的负载不均衡，有的重要性样本多，有的少，有的机器负载重，有的轻</font> 同构场景下也不一定要分配同样多的数据（利用调度思想解决：应该调度多少的样本到一个机器上，按照机器的剩余资源，总线占用来分配，调整batch_size lr/调度感知的通讯 or 梯度同步 or 根据topology结构来进行分组梯度同步）？进行梯度压缩通讯？进行分组异步通讯？异构集群下的通讯优化？算子融合？搜索一个最佳分配方案？超参数搜索的多job场景分布式通讯优化，减少冗余数据加载或搬运（结合Ray）？
  c. 数据加载时间减少，第一个batch 优先读取缓存+数据pipeline预取 
  xxx 感知的 xxxx
  eg. 压缩感知的梯度同步，其实就是因为感知，所以可以pipeline，时间覆盖实现加速

  参考论文：并行与分布式神经网络训练中数据通路的优化
  可以将重要性采样算法模式关闭，但是收益于其他的优化点（负载均衡/数据预取预缓存）！

2. 将重要性采样算法思想迁移到另外一个领域 (GNN?推荐系统？)
通过ali的负载分析，大量的任务要么是CPU 密集（GNN， 强化学习， CTR模型） 要么是GPU密集的，没有IO密集的。所以可以针对CPU密集场景设计重要性采样

3. 最优的参数，样本数量，数据增强的数量或者种类应该都是随着整个训练过程中动态变化的，而不是静止最优的（pollux论文启发）


### 题目暂定
基于样本重要性与参数重要性的深度学习训练加速研究
基于样本重要性的深度学习加速框架（数据增强，计算，通讯，IO）
基于池化平台的深度学习的任务调度和资源配置研究
提出指标->建立模型->用模型预测->选择最佳方案

知网参考论文：
GPU集群环境下基于流水线的分布式深度学习训练优化技术研究与实现（模型并行优化）
分布式深度学习系统基于模型结构特性的通信优化（分布式训练通讯优化）
对于卷积层,根据每一层的参数规模大小设置层自适应传输率,每次进行权值参数更新时仅选择一些重要的卷积核将其梯度传输给参数服务器,其余卷积核的梯度在本地累积至下一轮参数更新


### 论文模板
https://github.com/TheNetAdmin/zjuthesis

### 一些其他思考
建立每个样本梯度对最终精度（可能在不同训练阶段不同）影响的预测模型，用这个来进行阈值选择

SVP没有考虑到样本重要性是动态变化，删除就再也不训练
AutoAssist:需要另外训练一个模型来预测，而且实际上也是随机选择
biggestLoss为了达到最精确，每次都要重计算每个样本，为减少IO时间带来了困难；即使提出历史值方案，也最多只能重用很短2-3epoch时间

最开始profile时候计算每个样本的loss以及对模型精度的贡献？如何计算？
https://zhuanlan.zhihu.com/p/400344124 遗忘分数删减样本  建立 <font color=red>损失/M - 模型精度关系模型</font> 这样就可能将不同模型归为一类分析

每个样本对模型精度的影响，好像有个数据增强的论文提到过  就是图片方格的那篇论文

1. 打印出在不同模型精度下30% 40% 70% acc 下， 不同损失，内部每层梯度的值，求每层梯度的0的比例（或者总的），如果比例大于多少（90%），就说明该损失对模型梯度的影响几乎为0;
2. 或者直接引用别人如何定义损失和对模型影响两者的关系，拿来用


5-29 update
现有的重要性采样算法只关注计算，系统无感知，因此优化有局限
对现有重要性采样算法提出问题（无法动态调整）-> 改进算法+系统角度优化
存储端异步GraNd score计算+aggressive training prunning

重要值频繁更新-> 无用数据加载  不频繁更新-> subset 导致丢弃的样本无法再被计算  ===> 使用异步重要性更新+multi-metric+生命周期感知调整
冗余数据 ===> 减少数据加载，用一张图片的多个增强替代；
梯度重要性不同 ====> 不再等待一些不重要的梯度,收到超过半数的就返回结果（适应PS）/负载感知样本划分

框架一个bottleneck profiler+decision maker+算法模型插件方式加入框架 可关闭可开启可单可多metric


弱化重要性采样算法细节，参数设定等，改为使用模型/函数/强化学习来自动确定  突出其他改进


5-30 update
BytePS重读感想：
将一个实现一个目标的多种方案组合起来，比如PS和Ring-AllReduce 这样不会使得CPU资源浪费，同时也不会让CPU成为瓶颈，将不同类型node之间的通讯带宽都利用起来了，CPU-GPU GPU-GPU（混合最优）
找到通讯的瓶颈，节点与节点间通讯bottleneck大于节点内部之间的通讯，因此将节点内部数据进行sum汇总之后再进行节点间通讯，减少通讯数据量（分组通讯）
将一个动作拆分为多个动作，放在不同模块上执行（模块分离，解决CPU瓶颈）

池化优势：我感觉CPU interfer> GPU interfer可以使用池化解决，将GPU分给多任务，但是CPU分给单任务

data diet感想：
动态调整不重要样本和噪音样本的比例 判断样本噪音+重要性的准则结合起来作为multi-criterion

5-31
重要性采样算法可能会使得每个类别的数量不相等，可以打印出来看看分析本质原因
分析是否是模型无关的！还是数据的属性
多个网络选择出来的重要性分数更准确
自适应重要性以及噪音检测样本，通过验证集来优化
噪音分数指标+重要指标 一同来进行选择

Linear Mode Connectivity in Multitask and Continual Learning 看
...
# calculate summary statistics
data_mean, data_std = mean(data), std(data)
# identify outliers
cut_off = data_std * 3
或者梯度一直保持在较高很多轮


, our analysis in Fig. 2 suggests a simple and powerful method to prune data for optimal performance by optimizing just two hyperparameters of a sliding window using a validation set. 利用验证集去调节参数

重要性作为采样概率，阈值，比例三种采样方式

1. always, sometimes, and never samples 频率进行缓存
2. This suggests that a large portion of the dataset is composed of
samples that are difficult to rank

评价准则（专家系统进行噪音和重要性样本的分类  Mixture of experts）+剪枝过程+静态动态 都可以进行动态配置
当多个任务分配到同一个GPU导致内存不够的时候，使用重要性采样？？
滤波器剪枝+数据剪枝 

6-1
Accelerating Deep Learning with Dynamic Data Pruning
虽然是动态剪枝，也就是不同轮修剪的样本是不一样的，但是这篇论文是固定间隔将所有子样本的重要性进行计算后选择一个subset进行训练
在进行选择的时候使用了EMA（指数移动平均）+ e-greedy(随机从不重要样本以e概率采样) /Upper-confidence bound（均值方差） 的方法，需要设置一个alpha参数。通过分数阈值来进行计算，也是一个aggressive pruning rates。
但是问题：随机结果有时候表现比算法好，无法解决噪音问题,而且只是介绍了criterion，但是没说是阈值还是比例采样

Data Fine-pruning: A Simple Way to Accelerate Neural Network Training
prune-epoch和reg-epoch迭代进行，而且也是具有非常多的超参数，一个window包含多个prune-epoch+reg-epoch， 而且每个window最开始的reg epoch都是不一样的; 不同点在于：删除的是loss比较大的bad data本；因为每轮prune多少样本固定，所以可以计算出来时间；而且最后的加速效果只有14%左右

6-2:
Exploring the Memorization-Generalization Continuum in Deep Learning c-score
https://github.com/google-research/jax-influence 判断样本噪音，需要使用test data, 不知道会不会影响最后的加速效果
如何区分hard 和noisy的样本，因为hard 样本是有助于提高精度的，但是noisy不是

6-3：
遗忘分数高的样本肯能是困难，也可能是noisy 样本，如何区分它们
examples with noisy labels are among the most forgotten examples, along with images with “uncommon” features
 现在考虑使用方差，如果是noisy 那么应该波动更大，如果是hard那么损失应该逐渐下降；之后将hard样本+ noisy（对称noisy+非对称noisy）样本 单独提取出来进行分析 (mnist+cifar10)


 利用数据的重要性来优化其他：增强+batch梯度通信+或者其他与数据相关决定的值！！！！





### 毕业参考文献
https://zhuanlan.zhihu.com/p/400344124 （code NIPS 21） 
https://github.com/mansheej/data_diet 上文代码

通过阅读后发现Data Diet是：
提前运行多轮，求得每轮的GradN, forget score, l2-loss 然后进行平均后将mean score保留到一个路径之后进行读取来进行数据的选择；也就是需要先验的处理，同时只有最开始的时候使用了loadsubset，之后的训练过程全部使用的subset

https://scholar.google.com.hk/scholar?cites=6692350500928309521&as_sdt=2005&sciodt=0,5&hl=zh-CN 
Accelerating Deep Learning with Dynamic Data Pruning 提出用强化学习/主动学习来动态查找
Improving Contrastive Learning on Imbalanced Data via Open-World Sampling(新领域 code NIPS21)


### 之后打算

背景+相关工作一章

设计一章（框架图+每个部分图表，描述每个部分怎么做）

实验一章（暂时将之前的实验拿过来）

尽量将这个事情做深一点

6-4
分析不同epoch的噪音和hard样本的损失， 方差， 或者是forget event的关系
猜测hard example损失逐渐减少， forget event几乎为0
noisy样本的损失波动很大，因为它可能会被相似的样本学习，forget event会更高
Learning from Noisy Labels with Complementary Loss Functions 这篇论文中有hard和noisy样本的损失的关系图


6-16
之后毕业论文打算
1. 进行主要设计部分的编写
2. 添加流程图、 表
3. 添加初步实验数据

将重要性采样算法扩充到分布式训练可能会面临的问题
Furthermore, to scale out on multiple GPUs, PaGraph develops a fast GNN-computation-aware partition algorithm to avoid cross-partition access during data-parallel training and achieves better cache efficiency.

缓存的东西可能没用了，如何解决同时减少通讯时间


7.31
目前打算将去噪音部分加入重要性采样中
原来的重要性采样算法调整为低频+高频数据训练

算法：去噪 去冗余  重要性三个维度
底层：缓存，通讯压缩，负载感知调度

阅读论文：https://cloud.tencent.com/developer/article/1771809

支持profiler度量性能瓶颈并输出cpu mem, gpu利用率报告
支持以插件形式（score函数单独提出来） 1）增加删除采样算法，以及2）使用单标准或者多标准， 3）分段选择样本：冗余，噪音，重要性三重过滤 （参考k8s的实现原理）
支持智能决策系统（RL, 启发式）


