---
title: 每天任务记录
tags: daily task
categories:
  - 日常记事
date: 2021-11-15 11:19:12
---
11-15
记得之后验证一下multiple是否支持只有loss这种情况，如果支持的话，sampler全部修改为一个，同时目前只修改和看BS这种情况

tune参数包括warm_up_epoch and sampling rate


完成情况：整理完之前的IOAS代码，将multi和single进行融合，同时使用字典形式来替换if-else多个flag的情况
记录：leetcode 刷题指南完成：贪心算法， 哈希表，双指针部分, 栈与队列部分


11-16
初步完成论文框架思维导图

修改代码，找到IO瓶颈场景

在代码中添加上视频分析这个场景

今天突然想到suprevision是用的什么anchor box，需要将数据下载完成后验证，猜测使用featmap_size??而不是预设的anchor

总结：
看完interviewtop上操作系统的八股文
下载coco数据
完成一版xmind思路调整
查看阿里基础架构面经


11-17
找到io为瓶颈的场景，并进行重要性采样实验
目前发现MS比BS精度高

BS：面临的问题  如何选择阈值 目前：搜索出初始thres_ratio, 之后使用历史+当前thres_ratio调整，//没有被计算的使用远程存储的模型进行计算,以及根据模型能力调整thres_ratio

MS: recall interval,不同等级之间的recall 时间差距是1 epoch 吗，新划分出来训练的类又如何根据阈值划分呢（等分感觉不科学）//建一个模型自动划分和确定等待的时间  （多个阈值和多个等待时间的确定）

现在只修改BS (1)实现在存储端计算样本重要性， （2）感觉样本的位置

https://www.sigarch.org/the-new-bottlenecks-of-ml-training-a-storage-perspective/ 这个讲得挺好
处理的浏览了一下FreqCheck 的源码 但是每太明白细节上是如何在torch.save()后面封装 fsync(),   以及如何根据runtime overhead的profile结果进行自适应的checkpointing frequency超参数的调节

目标：弄懂多进程异步/模型的异步训练， 参考autoassist 以及checkFreq，subprocess
以及另外开一个进程来进行io带宽的测量(profile)以及未读取数据的重要性更新，另外一个空闲节点


11-18

看了autoassist的多进程，但是他们在训练另外一个模型进行样本的现在是训练了另外一个二分模型，这个二分模型的目标是 loss大于均值为target或者target model判断错误的label=1


我要训练的另外一个模型是在存储节点训练的模型，无需训练只需要将模型每次最新权重穿过来，在checkpoint的时候将模型传给远端; screen 需要同时训练另外一个模型会加重CPU和GPU训练负担，同时也不是很好同步轻量级模型和远程target模型判断样本重要性的差距

完成另外一个进程异步训练模型的初步代码

11-22

完成情况：
完成主进程和子进程相互传输重要性分数，invalid list以及模型初步代码

明天需要解决的问题：
1）如果解决模型参数保存的时候使用model.cpu().state_dict() 但是计算的时候需要使用dp 或者cuda, 保存模型这个动作如何转为复制模型参数，而不是真正将模型保存到cpu

2) 如何减少推理时间太长的问题，如果使用原模型进行不重要样本重要性计算

3）如果将一个节点转移到两个节点间的数据传输（远端存储系统情况）

4）把这个解决了，再解决如何修改io module 自定义一个缓冲区，然后在缓冲区/cache中实现TLB表，以及数据从缓冲区中读取



要解决的问题：1 分布式训练如何将另外一个进程计算得到的uni_score广播给其他进程
而且注意是多个分布式训练主进程对应一个子进程，不应该建立多个进行不重要样本重要性计算的子进程

目前遇到的问题：RuntimeError: module must have its parameters and buffers on device cuda:0 (device_ids[0]) but found one of them on device: cpu-->解决，因为model.cpu().state_dict导致


11-23

完成情况：

1）使用一个copy.deepcopy完成在gpu上模型的拷贝到cpu
2) 解决子进程无法正常退出的问题
3）初步熟悉dataloader和dataset的代码

下一步：
如何解决判断数据是在缓存， 本地磁盘， 还是远端存储系统呢

11-24  

1）调整了lru_cache的逻辑
2）但是目前不知道如何将tlb得到的信息转为io_predict_time
3）初步使用多进程访问同一个cache,同时记录总访问次数和命中率，因为多进程需要共享一个数据结构，因此使用multiprocessing.Manager 因为LRUcache是自定义的一个数据结构，因此需要使用自定义的manager

目前发现worker越多，access 缓存次数越多

worker = 8
self.size:256
total_access_time:2432, hit_time:128

worker = 4
self.size:256
total_access_time:1048, hit_time:128


worker = 0
self.size:128
total_access_time:384, hit_time:256

如何只是对cache的每个变量使用multiprocessing进行加锁的话会导致出现None 结构remove错误，所以对于cache的get set函数都应该是一个原子操作（一个事务），所以需要对cache使用原子操作（加锁），而不仅仅是里面的一个变量加锁就可以了

11-25

验证多进程为什么总的访问次数会增加，以及为什么使用缓存的大小增加，但是命中率会减小
考虑如何将tlb功能转为IO score的计算
如何感知数据的位置，在其他节点的存储中，本地存储中，还是缓存中

发现：epoch=0和epoch=1的时候多进程取数据hit time=0， cache size=256 但是如果worker=0的话， epoch=1时候的命中率应该为128 cache size=128
解释：因为我在执行流程中间break了，但是数据还在提前取后面一轮的，所以导致cache size会满，同时命中率下降；第二轮本来要取的序列就减少了，所以多进程也没有讲缓存的数据冲走；也可以解释第0轮epoch就有2000多次的访问量

加上缓存：
Training done in 0.069387 hours 
Training  done in 0.068791 hours


不加缓存：
Training  done in 0.017339 hours
Training  done in 0.035785 hours

Training  done in 0.037956 hours 命中率384

使用字典进行缓存，随机置换：
Training  done in 0.022509 hours 
Training  done in 0.039637 hours  命中率 224


pyshmht 为python设计的基于hash表的共享内存

asyncio

目前结论：使用一个随机置换策略的缓存相比LRU置换效果更好  或者直接使用coodl的缓存后就固定不变

感觉应该是LRU每次加锁读写链表

明日安排：完成整个训练过程加缓存以及如何将缓存的位置转化为IO score


11-29

RPC小例子
https://zhuanlan.zhihu.com/p/136372142

了解一下rpc pytorch， 打算使用nfs再使用orangefs来实现两个节点上worker的通信
看了两篇论文：1.是分析深度学习不同存储方式的性能 2.运用机器学习判断应用的IO模式，然后自适应的选择相应的优化策略（主要是针对有读写，以及I/O模式在整个应用生命过程中会改变，但深度学习是只读的，I/O模式似乎都是一样的）
https://github.com/apachecn/apachecn-dl-zh/blob/master/docs/pt-tut-17/65.md

11-30
使用sudo aptitude install nfs-common 选择n y的时候安装失败，导致sudo无法使用，同时机器无法连接，修机器
参加rebuttal的会议

12-1

问题如何在orangefs上挂imagenet, 挂了之后仍然像ext4文件一样访问 /mnt/orangefs吗， 记得修改默认orangefs 服务器的文件目录，目前默认是/local500/storage-orangefs/吗
最后为什么是mount hec09的orangefs 为什么不是local500/storage-orangefs


12-2 

1. 完成 rebuttal的提交
2. 初步在节点hec01 和hec03上实现 rpc测试  学长使用的是不同语言之间相互通信的grpc 先写一个.proto 文件，然后使用一个工具 将他编译为目标py文件或者go文件，然后分别在py和go文件里面使用这个编译好的文件

在提交英文版本的时候一定要注意英语的拼写问题

pytorch官网上的rpc网络： https://github.com/pytorch/pytorch/blob/master/docs/source/rpc.rst



12-3 现在的问题是：worker如何确定当前可以shutdown

目前遇到的问题是如果是将远程过程调用放在sampler，如果是多进程取sampler的话会导致 出错，因此应该把远程过程调用的逻辑放在主函数里面

多进程会有多个sampler副本吗  难道是多进程访问sampler会加锁？？
 


明天 1.实现在每个epoch前进行重要性计算 train_one_epoch前  2. score通过字典传递，而不是list

rpc在远程过程调用的时候，如果调用的函数和初始化不在main函数下面py文件(主要的train_main 文件里)内会导致调用函数不一样 本来打算调用worker, 调用到master端


12-7
实现使用nfs， 以及torch.distributed.rpc实现两个任务之间的通信， 阅读grpc 的route_guide example源码， 如果要使用grpc需要使用异步+call back函数，或者使用一个子进程+同步grpc，然后实现主进程和子进程之间通过共享内存实现数据的共享



12-13

在hec01使用setattr修改了文件dataset的属性
现在的问题是存储单元如何得知自己存储节点的数据，或者假设存储节点非常的近，那么存储节点只需要派一个节点进行训练就好了

现在可以实现在一个存储单元上挂载orangefs-client查看数据， 之后换到新机器上实验orangefs形式的存储单元和计算单元 通讯和传递消息，目前server端只需要一个节点来进行推理，不需要每个server对每个server端的数据进行推理

12-14 
找到一个bug， loss.mean().backward应该在loss.cpu.detach()前面，否则就会训练的时候精度一直为0，怀疑是torch版本的问题
打算再新环境中使用orangefs，然后进行通讯任务训练
以及考虑加上cache + IO score 预先训练cache中的数据


今天leetcode练习到532题

明日安排：
1. 先profile一下cache time 是否真的的比ssd time小很多 
2. 同时查看一下是不是每轮epoch 第一个iteration时间最长， 找到网上解答原因
3. multi-criterion到底应该怎么用。
4. 写一个简单的用cache的信息加入到multi-criterion计算（不同的优先级给定一个分数），以及在cache中采样第一个batch, 以及随机置换的cache策略，先写一个版本

5. cache size以及cache的转换策略将对io score产生怎样的影响

相同数据每次的load time是不是都是变化很大。我感觉应该用时间近似，而不应该用level,因为不同cache的ssd的远近可能导致这两个时间相差很大，有的ssd读取和cache时间差不多那么io score应该就差不多，否则就应该加大io score的差距


感觉多进程预取会抵消cache好处？？如果IO是瓶颈的话，那么预取的效果就比从cache中取数据的效果差


12-15 完成一个简易版本的cache-aware sampling

主要包含下面三个功能的实现：
1. IO score的计算问题 使用profile的cache的时间和ssd时间近似替代训练序列的all_result_list['iotime']
2. sampler的序列问题， 先使用cache 里面缓存的序列
3. cache的置换问题，初步使用self.vis字典实现先替换cache中vis的序列，但是问题是每个epoch都需要clear一下cache的vis序列

但是现在还是在注释中


12-16
尝试在8a100上实验，可能会考虑到分布式训练的sampler的修改
使之适应于分布式+orangefs这种场景

完成情况：目前在多GPU上调试了之前hec上面的代码，可以跑通单机多卡加上cache, 和修改了cache time计算方式的代码
初步完成昨天1， 2， 3个功能，目前的问题就是DataParallel+ another model in storage side可能会有问题

发现4 worker的时候 每4个iteration 第一个时间最长， 之后的时间都递减

明日计划：

四叉树合集有时间看看


12-17 数据传输慢是因为传输的带宽被占满了吗？ 是否可以开不同的数据取线程？来为用于级别cache增加命中率
为什么4worker时每个iteration 4个中必有一个时间长 （已解决）
这个iteration time和worker的设置有关


12-20
TODO: 分布式情况，多个节点得有多个cache, 这样的话在进行取数据的时候首先取cache中缓存的数据就不对，需要通过RDMA 需要直接取其他节点的cache数据才行
目前分析原因是消耗比读取快，消耗了一轮回过来，还是没有讲一个batch读完，所以又会在worker1处阻塞住，同时在worker1阻塞的时候，其他worker也在读取，所以表现为每隔一段时间就会出现一个IO时间非常长

但是如果计算是瓶颈，应该不会出现这个
resnet152的时候io_time就能够被计算时间覆盖了，所以所有的iotime大概都是0.0003左右


那么可以找到一个比较便捷的io瓶颈测量是将，看是否iotime是否随着worker进行周期变化； 但是总体上来说都是第一个iteration的iotime时间最长为1.6左右，因为没有任何计算来对iotime进行覆盖
alexnet其他worker每隔一段时间大概为0.5；resnet大概为0.2；其余为0.0003

https://kangsheng.xyz/2020/07/14/pytorch%E4%B8%ADdataloader%E7%9A%84num_workers%E8%AE%BE%E7%BD%AE/  
这里的iotime包括进程开销，以及batch读取+batch处理开销 但是多进程加载数据包括读+处理两个部分， 而不是只读
8A100 2个物理CPU, 每个cpu 物理64核， 每个核有两个逻辑处理器；所以两个cpu一起就有256个逻辑处理器（2*2*64）
hec02是8个cpu, 每个cpu上面只有一个核


https://zhuanlan.zhihu.com/p/91521705 查看dataset多worker加载数据，每个worker负责一个batch,只有在当前batch消耗掉之后才会取下一个分配的任务，而不是一直取数据。同时这个worker取数据包括读+数据增强两个部分
>其实各个worker之间读取数据时间差不多，并且由于是多进程任务，所以每次第一个worker读完数据就绪以后其他也准备就绪了，主进程随即开始连续前向传递了，并轮询的向各worker发送读新一批数据的信号，只不过跑完上一轮次所有worker产生的数据（num_workers个batch）后，各worker中新一轮次数据还没就绪，所以阻塞住了。

4worker的情况下；
io_time: 0.6680457592010498
io_time: 0.00039649009704589844
io_time: 0.0004131793975830078
io_time: 0.00040602684020996094
io_time: 0.7091846466064453


8worker的情况下：
io_time: 0.583601713180542
io_time: 0.00048732757568359375
io_time: 0.011454343795776367
io_time: 0.0005695819854736328
io_time: 0.00035643577575683594
io_time: 0.0003287792205810547
io_time: 0.0004932880401611328
io_time: 0.0003211498260498047
io_time: 0.5443663597106934


16worker的时候：
io_time: 0.6202142238616943
io_time: 0.0005917549133300781
io_time: 0.0004055500030517578
io_time: 0.00042128562927246094
io_time: 0.0004863739013671875
io_time: 0.00027370452880859375
io_time: 0.0002474784851074219
io_time: 0.0002865791320800781
io_time: 0.00033855438232421875
io_time: 0.00021886825561523438
io_time: 0.0002658367156982422
io_time: 0.00020456314086914062
io_time: 0.0001895427703857422
io_time: 0.0005667209625244141
io_time: 0.0004878044128417969
io_time: 0.013859033584594727

worker数量越小，出现io_time突然增加的可能性越高
现在的问题是：在worker数量不足的时候是否这种用另外的worker去取数据，是否会影响数据加载的性能；相当于增加了线程数量；这好像更多加一个worker，然后多加一个缓存空间，实现超前预取没有实质区别
另外一种合理的解释就是说利用data worker取数据的空闲时间来进行cache的换新，但是IO 瓶颈data worker空闲的时间也不多呀；只有计算密集的data worker才会空闲

而且cache如果这样设计的话和buffer没有什么区别，因为都是取一次用一次，结论在buffer里面实现预取进程无用! 而且可能增加磁盘带宽压力


coordl又讲使用cache的理由吗？感觉可以直接沿用coordl理由

Gist：在两次数据利用之间有长时间的gap，因此可以对数据进行压缩，之后要用的时候再解压缩， ==在cache里面对数据进行压缩解压== 问题是data没有稀疏性，如果较好的无损压缩


12-21
目前分别在hec 和 8a100上测试远程过程调用和cache的代码

看代码的时候注意一下几个方面：
1）远程过程调用现在不能使用dataparallel  create_file
2) cache代码里面可以使用DP, 需要确定cache大小
3）loss.backward一定要在loss.cpu/记录loss 之前，否则会报错
4）合起来的时候一定要注意将init_rpc 以及 有关worker和master的代码打开（train_evaluate）
5) 感觉增加worker数量就能消除IO瓶颈的问题如何解决

找到一个bug: 使用dp的时候，batch_size应该*gpu数量，因为是一个batch size拆分，但是ddp不是

明天任务：
如何实现多GPU情况下，多进程共享一个数据结构cache， 因为现在好像是多个进程，每个可能都会创建一个cache这个是不合理的

12.22 

看了以及整理了一下括号相关的leetcode题目

rpc在回调结果的时候遇到一个问题：
``` python
master使用远程过程调用的结果来更新criterion_history_list_dict
Traceback (most recent call last):
  File "./imagenet_final_train_master.py", line 206, in <module>
    train_main(args.gpu, ngpus_per_node, args, kwargs)
  File "./imagenet_final_train_master.py", line 94, in train_main
    run_training(model, trainloader, testloader, trainset, testset,  optimizer, criterion, dirs, args.epochs, args, kwargs)
  File "/nfs/home/shu/hushuang_all_files/ImageClassification/8a100-code/IOAS/imagenet/train_evaluate.py", line 177, in run_training
    refresh_uni_list( uni_score_rref, allscore_numpy_dict, args)
  File "/nfs/home/shu/hushuang_all_files/ImageClassification/8a100-code/IOAS/imagenet/rpc_func.py", line 44, in refresh_uni_list
    uni_score_list = uni_score_rref.to_here().numpy().tolist()
RuntimeError: RPCErr:1:RPC ran for more than set timeout (60000 ms) and will now be marked with an error

# worker 端

[W tensorpipe_agent.cpp:498] RPC agent for worker1 encountered error when sending response to request #16 to worker0: EOF: end of file
```


12.23
现在8a100端注释了master的和worker代码就可以实现 存储端计算重要性+主进程端使用cache缓存部分数据

主要逻辑：（cache 3+ 存储单元 1）
1. 0epoch冷启动， 在1epoch缓存端已经开始缓存数据， 每轮训练的时候将vis的数据打一个标记，置换策略将vis的数据随机置换出去
2. 下一轮 sampler返回训练的时候将cache中的数据放在前面
3. 在重要性采样开始前一轮epoch，开始搜集重要性metric设计的分数，如果有io 分数的话，通过profile 的iotime 来对cache time的ssd load time进行估计（求平均值），同时对下一轮样本的iotime进行预测
4. 将计算出来的重要的样本下一轮训练，不重要的样本序列通过rpc传给存储端的计算单元，让master和worker分别更新重要的样本和不重要样本的分数。
5. 然后之后在进行采样的使用用全部更新好的重要性分数，重复排序和训练更新步骤


解决两个bug:
1) 之所以8a100上面使用分布式调用不起来，是因为我之前在使用torch.distributed.rpc的时候写了os['environment'] =hec01  导致分布式错误
2) 训练精度很低， 修改criterion
criterion = nn.CrossEntropyLoss(reduce=False).cuda(args.gpu) hec02实验
目前是用这个解决了，但是assert语句显示一致, 不知道原因


12-27
解决cache在mp.spawn 使用多进程训练的时候导致段错误或者信号接收错误
选择启动方式：mp.set_start_method('spawn')

在进行调试的时候，break不应该放在args,rank==0下面，否则就会卡死

12-28
现在的任务就是看rpc是否能和分布式训练的mp.spawn同时存在, 应该主进程主要负责rpc

ssh需要输入密码的节点为何无法使用rpc进行通信


12-29
今天主要看了legoos这篇论文， 对调度相关的其他论文也进行了总结和分类，以及看了ppt下面的链接有关内容
明日安排，做一下最短路径（单源或者多源情况）， 练习今天学到的有关Dijstra, Bellman-Ford算法以及Floyd算法


#### 2022
1-3
完成>400目标
代码主要就是看分布式+rpc是否能够兼容(+cache目前通过mp.spawn解决)并调试通过，实在ddp+rpc无法完成，就只能使用dp+rpc


1-6 现在遇到的问题：1.有ssh密码登录的机器，使用rpc无法正常运行， 同时rpc在同一台机器好像也无法正常运行

解决！
注意在rpc中发出rpc_async请求的是worker, 接收请求的是master

hec上使用pytorch 1.8.1只能在没有gpu的节点上，如果在hec01 hec02上就会报错

现在问题 8a100和hec06节点连接会报错：
aster: ECONNREFUSED: connection refused
Failed to respond to 'Shutdown Proceed' in time, got error ECONNREFUSED: connection refused
Traceback (most recent call last):
  File "worker.py", line 88, in <module>
    rpc.shutdown()
  File "/nfs/home/anaconda3/envs/pytorch-shu/lib/python3.6/site-packages/torch/distributed/rpc/api.py", line 78, in wrapper
    return func(*args, **kwargs)
  File "/nfs/home/anaconda3/envs/pytorch-shu/lib/python3.6/site-packages/torch/distributed/rpc/api.py", line 284, in shutdown
    _get_current_rpc_agent().join()
RuntimeError: [/opt/conda/conda-bld/pytorch_1607370193460/work/third_party/gloo/gloo/transport/tcp/pair.cc:575] Connection closed by peer [10.5.30.42]:27806

Solution: Edit /etc/hosts and change 127.0.1.1 to your actual ip-address 目前看到的一个答案
分析：现在在hec上互相使用rpc（即使有密码）是可以的，但是在8a100就不行，所以现在怀疑的是需要在host里面添加目录，以及需要在两台机器上换成同一个用户才行
第二种解决方案，应该使用tpc的init_method，以及使用gloo后端，但是目前试验过，仍然会报错

感觉在分布式多台机器上训练也会出现相同的问题

找到的真正的解决方案：backend=rpc.BackendType.PROCESS_GROUP， 不需要加其他的
os.environ['TF_SOCKET_IFNAME'] = 'ens3' (x)
os.environ['GLOO_SOCKET_IFNAME'] = 'ens3' (x)


寒假安排：
1. leetcode争取上500 可以先将宫水三叶的题刷完，应该都是经典题目
2. 对之前写的leetcode进行总结，整理，转为每个知识点，先放总结的思路，或者针对的题型，然后放相应的例题，没道例题简答写一下解决方案，和总结的思路进行对应
3. 回归操作系统，计网的基础知识, 转为笔试的xmind (408题目看看) 
github和牛客网应该有许多资源，多看看多比较; 包括深度学习和目标检测相关的面试题目
牛客网上， 网友的总结：(CyC2018)
[https://github.com/CyC2018/CS-Notes/blob/master/notes/%E8%AE%A1%E7%AE%97%E6%9C%BA%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%20-%20%E7%9B%AE%E5%BD%95.md]
4. 补充面试的xmind
5. 有时间学一下grpc
6. 记得备份一下8a100上面的数据 (目前正在8a100使用rsync进行传输 到hec06 /nfs目录下)
7。一些公司名单和深度学习的资料
https://github.com/amusi/CV-Company-List
注意关注一下春招信息，微软已经开始投递了，然后我因为错过了问卷，错过了提前批，之后注意准备面试和之后的问卷
8. 修改简历，变为项目两个， research两个，将重要性采样自己的工作写上去zzw
9. 项目一定要非常非常熟悉，整理面试可能问到的每一个问题，和自己优化的思考！！！


刷完八股之后，进行自测：https://github.com/youngyangyang04/TechCPP
https://github.com/youngyangyang04/leetcode-master

有关于C++的八股文：https://github.com/Light-City/CPlusPlusThings



408资料
https://github.com/CodePanda66/CSPostgraduate-408
408历年真题大题查看


公司：思科，


别人的面经总结：
https://prohuper.github.io/2020/04/17/interview_exp2/

推荐资料:
力扣1-300题（前300道题非常经典，建议学有余力的同学都刷一刷）
力扣HOT100（跟上面有不少是重复的，刷的时候要注意总结）
啊哈！算法、大话数据结构（这两本书都是面向新手的图书，图画很多）
剑指offer（这本书不需要多做介绍，校招必备）
挑战程序设计竞赛（这本书属于进阶一点的算法书籍了，作者是ACM-ICPC全球总冠军，可以说是
世界顶级程序设计高手的经验总结了，需要慢慢消化，经典题型太多）
程序员代码面试指南（左程云大神的书，我并没有看完，只是看了其中的海量数据处理部分的题
目就已经十分受用了，在某大厂三面中就考查到了其中的海量数据集处理的问题）


按公司分题：https://github.com/afatcoder/LeetcodeTop（面试前刷一刷）

https://github.com/HarleysZhang/2021_algorithm_intern_information 计算机视觉-算法岗位相关
https://github.com/DWCTOD/interview


把leetcode的剑指offer开头 和leetcode全部看一篇




5-11
### 有关于毕业论文的任务：
找个时间整理一下之前重要性采样的工作，评估是否能够作为毕业论文；如果不能，找一下有关于k8s调度方面的论文，尝试将现有的工作转为毕业论文
仿照学长的论文，搭出一个初步的论文框架，然后缺的内容和实验，利用空余时间补充，对比实验和代码都需要整理一下
确保自己能够毕业！！！
可以考虑向自己的论文中添加一下几个方面的内容，增加难度：
1. 结合 Ray 超参数 HyperOpt 自动调节超参数的工作，（类似adaptDL）
2. 创新重要性样本计算的过程：多级反馈队列+异步重要性更新
3. 增加在分布式里面的实验
4. 必要时看看之前oneNote里面的笔记

始终记住敏捷开发方法！先写个初步原型，后面逐渐精细化

### 有关于调度器framework的扩展方法：
一些学习调度框架插件扩展的例子：

一个简单的例子：
https://github.com/cnych/sample-scheduler-framework

https://github.com/angao/scheduler-framework-sample(有makefile文件)

一个稍微复杂的例子：
https://github.com/Mr-Linus/Yoda-Scheduler

一个更加复杂的例子
https://github.com/kubernetes-sigs/scheduler-plugins/blob/master/doc/install.md
1. 将自己写的调度器作为第二个调度器
2. 将自己写的调度器作为唯一一个调度器，删除掉默认的调度器 

一个简单的例子，不知道能不能运行：
https://github.com/lwabish/k8s-scheduler

<font color=red>重点理解</font>
原生k8s default-scheduler的调度算法和代码 框架图，以及默认插件等（比较全面）
https://shikanon.com/2021/%E6%9E%B6%E6%9E%84/k8s%E8%B0%83%E5%BA%A6%E5%99%A8%E6%BA%90%E7%A0%81%E8%A7%A3%E6%9E%90/

pkg/scheduler/algorithmprovider/registry.go //原生提供的一些插件
pkg/scheduler/core/generic_scheduler.go 通用调度算法代码的位置
原生代码中也调用了过滤和打分插件
https://www.cnblogs.com/luozhiyun/p/13619296.html

pkg/scheduler/framework/types.go 主要定义了之后可能需要用的struct的信息
pkg/scheduler/framework/plugins 原生的一些扩展插件，具体实现可以看看

Kt Connect  k8s如何在本地进行debug
https://cloud.google.com/code/docs/vscode/debug-k8-pod?hl=zh-cn

https://github.com/kubernetes/community/tree/master/contributors/devel#readme 官方的development文档

https://github.com/kubernetes/community/blob/master/contributors/devel/README.md#sig-testing 
查看SIG Scheduling 这一栏的数据

kubectl top node/pod 可以获取节点的信息，需要下载metric-apiserver

https://blog.csdn.net/weixin_39169791/article/details/105098481 原生调度器调试过程 （关键词：本地调试，断点调试） 之前按照这个配置

https://blog.wubw.fun/2021/07/kubernetes-scheduler-framework-study-2.html 原生调度插件功能介绍

https://duyanghao.github.io/kubernetes-remote-debug/ k8s使用goland进行远程调试的步骤

最后按照： https://www.bilibili.com/video/BV1Qy4y1K7nk/ 视频实现vscode远程调试k8s,但是一直出现 connect to 127.0.0.1:2345错误，用
go mod vendor解决，主要是因为需要更新go.mod的信息，另外还没有尝试的解决方案包括： export GOFLAG="-mod=vendor"

（5-25）查看默认的filterPlugions有哪些，以及进行调试
kubernetes/pkg/scheduler/framework/runtime/framework.go
for _, pl := range f.filterPlugins {
		pluginStatus := f.runFilterPlugin(ctx, pl, state, pod, nodeInfo)
}
自己的定义的调度器会以deployment的形式部署，然后新建一个pod使用这个调度器，所以自定义的调度器如何调试呢




### k8s调度器源码 以及 vODLA调度器源码查看笔记
基于池化的调度器：
	- 感觉这个仓库里面的调度器相比于原生或者其他自定义的k8s的调度器的区别在于多个一个vodla handler， 这个handler是一个包含deviceCache的数据结构，该deviceCache数据结构可以通过cache增删改查 ovdla设备（主要还是通过一个vodla的controller，该代码暂时不可访问）；该handler可以调用函数 getClaim 和getDevices ，GetNodesWithUnconsumedDevice 相当于封装了cache, 与外面调用device的函数（PreFilter/Filter/Score）交互

	- 这个调度器只是写了PreFilter和Filter信息，没有score相关的函数


原生调度器信息：
	- 只有framework的handler(只打印的版本)
	- 如果需要监控其他信息吗,可能会有其他的handler 或者cache来进行数据记录 比如scv， cpu, gpu, memory信息


那么k8s scheduler框架的调度器提供的 node *framework.NodeInfo 和 vodla handler 提供device信息有什么区别和联系呢
池化调度器只是使用到了nodeInfo.Node().Name这一个信息
好像vodla_device可以还原到到底是哪个node信息（node_id等）

问题：vodla device是什么样的格式，提供什么信息；为什么vodla设备会被分配给node



#### 有关extender的扩展方法
6-5 
判断自己应该修改k8s哪个部分文档
https://kubernetes.io/zh/docs/concepts/extend-kubernetes/_print/

extender的部分
别人博客的描述：
https://liqiang.io/post/kubernetes-scheduler-extender-dd6516a6

官方文档的描述：
https://github.com/kubernetes/design-proposals-archive/blob/main/scheduling/scheduler_extender.md



你只需要在 HTTP 处理代码中反序列化成这个结构体，然后执行自己的逻辑

http服务将得到的json文件进行反序列化，然后用自己熟悉的语言实现filter函数，然后按照要求返回结果字段
并将结果序列化之后，返回response

可以运行的sample：
https://liqiang.io/post/kubernetes-scheduler-extender-dd6516a6 go + http server代码解析描述见，重要；需要修改默认调度器（而且修改默认调度器的方法应该是不一样的，可以再找找看）
https://github.com/everpeace/k8s-scheduler-extender-example 使用go + http server实现的代码, 不需要修改默认的调度器
 

https://github.com/AliyunContainerService/gpushare-scheduler-extender/tree/878b6fde505d6e11fd15ef4fdcc67091b6d6323a/pkg/scheduler 阿里云的一个扩展器例子

在github上搜索（explore） scheduler-extender 其他例子

首先需要查看http server实现的官方文档
用go语言实现一个http 服务器：https://segmentfault.com/a/1190000021653550
用python实现http服务器：https://bearzpy.github.io/2017/03/23/Python/%E7%94%A8%20Python%20%E5%AE%9E%E7%8E%B0%20Http%20%E6%9C%8D%E5%8A%A1%E7%AB%AF/index.html







#### 总结
两种方法总结:

**扩展程序**：
- 通过编写一个http服务 ,独立启动，运行在extender制定的端口上，使用GOOS=linux GOARCH=amd64 go build -o app;然后在节点上运行 ./app进行执行，相当于启动了一个http server的服务器
- 在原生Scheduler 的运行参数中加入一个：--config = xxxx extender-config.yaml
(相当于另起一个http服务的pod, 而且新加的调度是在原生调度后面再进行一次过滤，predicate 在原生pred后执行；priorize在原生调度器的priorize执行后执行)
- 起一个测试任务
- 感觉之后所有的任务在调度时候都会包含 sample-scheduler-extender了

重要！上面这种方法修改了默认的调度器，所以所有的调度都会使用extender
另外一种以deployment的方式启动scheduler-extender的方法：
github.com/everpeace/k8s-scheduler-extender-example 这种方式类似于调度框架的方法，不需要修改默认的kube-scheduler.yaml的参数, 这样每需要每次在pod中指定schedulerName： my-scheduler

**调度器框架**：
- 扩展插件接口，编写调度器插件的代码 pkg/plugins
- 然后使用命令```kubectl apply -f deploy/sample-scheduler.yaml``` 手动启动一个调度器pod
- 起一个测试任务，在yaml文件中使用sampler-scheduler-framework




















