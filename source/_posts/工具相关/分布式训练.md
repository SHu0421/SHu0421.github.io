---
title: 分布式训练
categories:
  - 工具相关
date: 2022-01-26 10:50:50
tags: 分布式
index_img: https://img.xjh.me/random_img.php?type=bg&ctype=nature&return=302
---

### 资料：
一个pytorch 分布式 单机多卡/多机多卡的例子
https://github.com/lesliejackson/PyTorch-Distributed-Training


常见分布式训练讲解，但是主要命令是启动单机多卡，而不是多机多卡 包括slum, apex, horovod
https://github.com/tczhangzhi/pytorch-distributed 

pytorch DDP介绍
https://zhuanlan.zhihu.com/p/76638962

https://github.com/richardkxu/distributed-pytorch （也有多机多卡的命令）

可能会用到：
两个机器之间相互免密登录：https://blog.csdn.net/u010391029/article/details/51126210


DDP time breakdown
https://discuss.pytorch.org/t/how-to-measure-ddp-time-breakdown/78925
https://discuss.pytorch.org/t/how-to-measure-ddp-time-breakdown/78925
https://discuss.pytorch.org/t/calculating-flops-of-a-given-pytorch-model/3711/4 FLOPS计算


多进程logging和mkdir设置，分布式inferenc/ 分布式训练的一些小技巧

https://zhuanlan.zhihu.com/p/250471767 

> 包括的内容1. 在DDP中引入SyncBN
> 2. DDP下的Gradient Accumulation的进一步加速
> 3. 多机多卡环境下的inference加速
> 4. 保证DDP性能：确保数据的一致性
> 5. 和DDP有关的小技巧
> 6. 1. 控制不同进程的执行顺序
>    2. 避免DDP带来的冗余输出

Ray分布式执行框架  PyTorch + Ray Tune 调参
https://blog.csdn.net/tszupup/article/details/112059788

分布式相关论文源码
https://github.com/msr-fiddle/pipedream  pipedream源码 mini-bath parallism
https://github.com/kakaobrain/torchgpipe

pytorch分布式训练的时候保存一定要用model.module
https://blog.csdn.net/comway_Li/article/details/107531165 一些需要注意的地方


horovod多机多卡
http://chaopeng.name/2020/01/03/horovod%E5%A4%9A%E6%9C%BA%E5%A4%9A%E5%8D%A1%E5%90%AF%E5%8A%A8%E6%8C%87%E5%8D%97/


### 知识点
1. DP和DDP区别
DP是单进程多线程数据并行，将模型copy到每个设备，scatter数据到每个设备进行计算后，将多个GPU输出传到master device(一般为设备0)进行损失计算，（可以优化一下：将loss作为forward 一部分，这样的话每个GPU上自行计算loss，而不用进行模型输出的传输）将损失结果（分发的是各个GPU分别对应的loss）传给每个设备进行反向传播，每个设备将反向传播后的梯度到主设备进行汇聚求平均，然后replicate一份模型的参数到每个设备中，重复上诉的训练过程。


2. DDP中all reduce与PS
    DDP （all reduce）同步的是梯度，不是参数
    PS，传给master的是梯度，返回的是参数

    思考：对不重要的梯度进行累积后同步/直接不同步（会不会造成模型的不一致）

3. 同步代码
分布式训练源代码解读：https://zhuanlan.zhihu.com/p/343951042
```
手动同步梯度
for param in ddp_model.parameters(): 
	dist.all_reduce(param.grad.data, op=dist.reduce_op.SUM)
    param.grad.data /= world_size 
	
手动同步参数：
在不重要的样本反向传播后但是不同步参数后，同步梯度
for param in ddp_model.parameters(): 
	dist.all_reduce(param.data, op=dist.reduce_op.SUM)
    param.data /= world_size 
```

### 例子
#### solution 1 - atom版本
裸机 多节点 atom平台
```bash
cd /workspace/mnt/storage/anchao/ac_file/traffic_iteration/SupreVision && \
./install.sh && \
python -m torch.distributed.launch \
        --nnode=4 \
        --node_rank=0 \
        --nproc_per_node=8 \
        --master_addr="10.244.55.30" \
        --master_port=23467 \
        tools/train.py \
        --config_file configs/YOLOv3/opencv_pelee_reorg.yml \
        SOLVER.IMS_PER_BATCH "16" \
        MODEL.DEVICE_ID "('0,1,2,3,4,5,6,7')" \
        > detection0.log 2>&1
```

单机多卡
```bash
cd /workspace/mnt/storage/anchao/ac_file/traffic_iteration/SupreVision && \
./install.sh && \
CUDA_VISIBLE_DEVICES=3,4,5,6 python -m torch.distributed.launch \
                --nproc_per_node=2 \
                --master_port=$((RANDOM + 20000)) \
                ./imagenet_final_train_master.py \
                --config_file configs/YOLOv3/opencv_pelee_reorg.yml \
                > detection0.log 2>&1
```

#### solution 2 - 简洁版
多机多卡
suppose we have two machines and one machine have 4 gpus
In multi machine multi gpu situation, you have to choose a machine to be master node.
we named the machines A and B, and set A to be master node
```bash
script run at A

python -m torch.distributed.launch --nproc_per_node=4 --nnode=2 --node_rank=0 --master_addr=A_ip_address master_port=29500 main.py ... 

script run at B

python -m torch.distributed.launch --nproc_per_node=4 --nnode=2 --node_rank=1 --master_addr=A_ip_address master_port=29500 main.py ... 
```

单机多卡
```bash
python -m torch.distributed.launch --nproc_per_node=ngpus --master_port=29500 main.py ... 
```