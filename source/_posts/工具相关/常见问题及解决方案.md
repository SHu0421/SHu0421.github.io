---
layout: blog
title: 常见问题及解决方案
date: 2021-11-11 16:48:29
tags:  vscode python
---

1. vscode调试的时候找不到路径

- 解决方案:在运行的python文件中添加sys.path，同时注意vscode调试的环境可以通过vscode界面最下面一栏进行替换
```
import sys
sys.path.append("/root/hushuang/IS-code")
```

2. 程序运行的时候报错 no module named XXX
- 如果是引入了与当前包并行的其他包里面的python文件，需要对文件使用setup tools进行打包，才能看到其他module里面的python文件
```
setup.py
    from setuptools import setup, find_packages

    setup(
        name='IO-aware Importance Sampling',               # 应用名
        version='1.0',              # 版本号
        packages=find_packages(),   # 包括在安装包内的Python包
        include_package_data=True,   # 启用清单文件MANIFEST.in
        # exclude_package_date={'':['.gitignore']},
        # install_requires=[          # 依赖列表
        #     'Flask>=0.10',
        #     'Flask-SQLAlchemy>=1.5,<=2.1'
        # ]
    )

install.sh
    python setup.py 
//生成的XXX.egg.info文件是和install.sh一个目录内， build文件夹 是和setup.py同一个目录内
```
- 在import的时候不能导入整个包名 比如IOAS/A/a.py 想要访问IOAS/B/b.py的函数时候，不能在a.py函数里面写 from IOAS.B.b import func_XXX; 正确应该是 from B.b import func_XXX; 


3. vscode无法连接远程服务器，但是能够ssh

解决方案：服务器/home目录满，清理之后就可以了
方法二：清理~/.vscode-server 删除就可以了

4. python 出现no module find XX

一般是一个python文件引用了同一个文件夹里面另外一个python文件，解决方案就是添加一个setup文件，同时一定要注意install文件和setup文件要放在整个目录的最外层 IOAS里面包含的第一层

如果出现函数参数和报错信息不一致的话，可能是因为setup安装了两个同名的文件，导致出错，使用pip unistall 自己的项目文件名称就可以了


5. 在使用RPC的时候发现如下问题
>  uni_score_rref = rpc.rpc_async("worker", get_uni_sample_score_rref, args=(rank, uni_list_rref, model_param_rref, args, ))
  File "/nfs/home/shu/.local/lib/python3.6/site-packages/torch/distributed/rpc/api.py", line 77, in wrapper
    return func(*args, **kwargs)
  File "/nfs/home/shu/.local/lib/python3.6/site-packages/torch/distributed/rpc/api.py", line 635, in rpc_async
    return _invoke_rpc(to, func, RPCExecMode.ASYNC, args, kwargs)
  File "/nfs/home/shu/.local/lib/python3.6/site-packages/torch/distributed/rpc/api.py", line 481, in _invoke_rpc
    PythonUDF(func, args, kwargs)
  File "/nfs/home/shu/.local/lib/python3.6/site-packages/torch/distributed/rpc/internal.py", line 86, in serialize
    p.dump(obj)
TypeError: can't pickle _thread.RLock objects

解决：去掉调用rpc函数里面的args参数，因为args参数里面有一个logger 会导致报错



6. The results is different when placing rpc_aync at a different .py file


I want to execute a function at the worker side and return the results to the master. However, I find that the results is different when placing rpc_async at a different .py file

#### Method 1

master.py:

``` python

import os
import torch
import torch.distributed.rpc as rpc
from torch.distributed.rpc import RRef
from test import sub_fun

os.environ['MASTER_ADDR'] = '10.5.26.19'
os.environ['MASTER_PORT'] = '5677'

rpc.init_rpc("master", rank=0, world_size=2)
rref = torch.Tensor([0])
sub_fun(rref)
rpc.shutdown()
```

test.py

```python

def f(rref):
print("function is executed on master")

def sub_fun(rref):
x = rpc.rpc_async("worker", f, args=(rref,))
```

worker.py:

```python
import os
import torch
import torch.distributed.rpc as rpc
from torch.distributed.rpc import RRef

os.environ['MASTER_ADDR'] = '10.5.26.19'
os.environ['MASTER_PORT'] = '5677'

def f(rref):
print("function is executed on worker")
rpc.init_rpc("worker", rank=1, world_size=2)
rpc.shutdown()
```

I found that the output is "function is executed on master" at the worker side.

#### Method 2

when I put the two functions: sub_fun and f in the master.py rather than the test.py, the result is "function is executed on worker".

Why the two ways output the different results. and how can I get the result 2 with the method 1.


7. args 删除里面的某个参数
```python
  logger_arg = args.logger
  del vars(args)['logger']

  print('--------args----------')
  for k in list(vars(args).keys()):
      print(k)
      # print('%s: %s' % (k, vars(args)[k]))
  print('--------args----------\n')
```


8. torch.distributed.rpc.rpc_aync
rref相关文档 https://m.w3cschool.cn/pytorch/pytorch-cdva3buf.html

https://github.com/pytorch/pytorch/issues/26759

9. 修改账户sudo权限
在有sudo权限的账户下执行 sudo visudo
并在下面添加以下权限
zhinengjisuan ALL=(ALL)     ALL


10. how to share a cache among multiple subprocesses when using PyTorch DDP training.

I want to share a cache among multiple processes when using ddp training. 
I found a potential solution is to use shared memory with torch.multiprocessing. 
However, it is not convinient if I want to training on multiple nodes, thus I choose the method ```torch.distributed.launch``` rather than ```mp.spwarn``` to init DDP training. 
The question is how can I share a cache among multiple subprocesses in the same node,  when using the method ```torch.distributed.init_process_group```. 
Supposed that there are N nodes, we should create N caches, and these subprocesses in the same node share one of the caches.

Has someone encounter the same problem before?

已解决:
mp.set_start_method('spawn')


11. /nfs/home/shu/.local/lib/python3.6/site-packages/PIL/TiffImagePlugin.py:788: UserWarning: Corrupt EXIF data.  Expecting to read 4 bytes but only got 0.
  warnings.warn(str(msg))

solution 1:
```python
import os
from PIL import Image

imageFolder = /Path/To/Image/Folder
listImages = os.listdir(imageFolder)

for img in listImages:
    imgPath = os.path.join(imageFolder,img)
            
    try:
        img = Image.open(imgPath)
        exif_data = img._getexif()
    except ValueError as err:
        print(err)
        print("Error on image: ", img)
```

12. 8a100连接hec报错
Torch RPC Connection closed by peer 
[W tensorpipe_agent.cpp:942] RPC agent for worker encountered error when sending outgoing request #0 to master: ECONNREFUSED: connection refused
[W tensorpipe_agent.cpp:942] RPC agent for worker encountered error when sending outgoing request #1 to master: ECONNREFUSED: connection refused


真正的解决方案：backend=rpc.BackendType.PROCESS_GROUP， 
不需要加其他的 os.environ['TF_SOCKET_IFNAME'] = 'ens3' 或者os.environ['GLOO_SOCKET_IFNAME'] = 'ens3'

