

<!DOCTYPE html>
<html lang="zh-CN" data-default-color-scheme=auto>



<head>
  <meta charset="UTF-8">
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@500;600;700&display=swap" rel="stylesheet">
  <link rel="apple-touch-icon" sizes="76x76" href="/img/favicon.png">
  <link rel="icon" href="/img/favicon.png">
   <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=5.0, shrink-to-fit=no">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  
  <meta name="theme-color" content="#2f4154">
  <meta name="description" content="这是我的第一篇博客">
  <meta name="author" content="Shu">
  <meta name="keywords" content="">
  <meta name="description" content="11-15记得之后验证一下multiple是否支持只有loss这种情况，如果支持的话，sampler全部修改为一个，同时目前只修改和看BS这种情况 tune参数包括warm_up_epoch and sampling rate 完成情况：整理完之前的IOAS代码，将multi和single进行融合，同时使用字典形式来替换if-else多个flag的情况记录：leetcode 刷题指南完成：贪心算法">
<meta property="og:type" content="article">
<meta property="og:title" content="每天任务记录">
<meta property="og:url" content="http://example.com/2021/11/15/%E6%97%A5%E5%B8%B8%E8%AE%B0%E4%BA%8B/%E6%AF%8F%E5%A4%A9%E4%BB%BB%E5%8A%A1%E8%AE%B0%E5%BD%95/index.html">
<meta property="og:site_name" content="Hello World">
<meta property="og:description" content="11-15记得之后验证一下multiple是否支持只有loss这种情况，如果支持的话，sampler全部修改为一个，同时目前只修改和看BS这种情况 tune参数包括warm_up_epoch and sampling rate 完成情况：整理完之前的IOAS代码，将multi和single进行融合，同时使用字典形式来替换if-else多个flag的情况记录：leetcode 刷题指南完成：贪心算法">
<meta property="og:locale" content="zh_CN">
<meta property="article:published_time" content="2021-11-15T03:19:12.000Z">
<meta property="article:modified_time" content="2023-07-12T12:10:28.257Z">
<meta property="article:author" content="Shu">
<meta property="article:tag" content="daily task">
<meta name="twitter:card" content="summary_large_image">
  
  <title>每天任务记录 - Hello World</title>

  <link  rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4/dist/css/bootstrap.min.css" />


  <link  rel="stylesheet" href="https://cdn.jsdelivr.net/npm/github-markdown-css@4/github-markdown.min.css" />
  <link  rel="stylesheet" href="/lib/hint/hint.min.css" />

  
    
    
      
      <link  rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@10/styles/github-gist.min.css" />
    
  

  
    <link  rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3/dist/jquery.fancybox.min.css" />
  


<!-- 主题依赖的图标库，不要自行修改 -->

<link rel="stylesheet" href="//at.alicdn.com/t/font_1749284_ba1fz6golrf.css">



<link rel="stylesheet" href="//at.alicdn.com/t/font_1736178_kmeydafke9r.css">


<link  rel="stylesheet" href="/css/main.css" />

<!-- 自定义样式保持在最底部 -->


  <script id="fluid-configs">
    var Fluid = window.Fluid || {};
    var CONFIG = {"hostname":"example.com","root":"/","version":"1.8.12","typing":{"enable":true,"typeSpeed":70,"cursorChar":"_","loop":false},"anchorjs":{"enable":true,"element":"h1,h2,h3,h4,h5,h6","placement":"right","visible":"hover","icon":""},"progressbar":{"enable":true,"height_px":3,"color":"#29d","options":{"showSpinner":false,"trickleSpeed":100}},"copy_btn":true,"image_zoom":{"enable":true,"img_url_replace":["",""]},"toc":{"enable":true,"headingSelector":"h1,h2,h3,h4,h5,h6","collapseDepth":0},"lazyload":{"enable":true,"loading_img":"/img/loading.gif","onlypost":false,"offset_factor":2},"web_analytics":{"enable":false,"baidu":null,"google":null,"gtag":null,"tencent":{"sid":null,"cid":null},"woyaola":null,"cnzz":null,"leancloud":{"app_id":null,"app_key":null,"server_url":null,"path":"window.location.pathname"}},"search_path":"/local-search.xml"};
  </script>
  <script  src="/js/utils.js" ></script>
  <script  src="/js/color-schema.js" ></script>
<meta name="generator" content="Hexo 5.4.0"><link rel="alternate" href="/atom.xml" title="Hello World" type="application/atom+xml">
</head>


<body>
  <header style="height: 60vh;">
    <nav id="navbar" class="navbar fixed-top  navbar-expand-lg navbar-dark scrolling-navbar">
  <div class="container">
    <a class="navbar-brand" href="/">
      <strong>HU&#39;s Blog</strong>
    </a>

    <button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse"
            data-target="#navbarSupportedContent"
            aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
      <div class="animated-icon"><span></span><span></span><span></span></div>
    </button>

    <!-- Collapsible content -->
    <div class="collapse navbar-collapse" id="navbarSupportedContent">
      <ul class="navbar-nav ml-auto text-center">
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/">
                <i class="iconfont icon-home-fill"></i>
                首页
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/archives/">
                <i class="iconfont icon-archive-fill"></i>
                归档
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/categories/">
                <i class="iconfont icon-category-fill"></i>
                分类
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/tags/">
                <i class="iconfont icon-tags-fill"></i>
                标签
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/about/">
                <i class="iconfont icon-user-fill"></i>
                关于
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item dropdown">
              <a class="nav-link dropdown-toggle" target="_self" href="javascript:;" role="button"
                 data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">
                <i class="iconfont icon-books"></i>
                爱好
              </a>
              <div class="dropdown-menu" aria-labelledby="navbarDropdown">
                
                  
                  
                  
                  <a class="dropdown-item" href="/favorite/">
                    
                    概述
                  </a>
                
                  
                  
                  
                  <a class="dropdown-item" href="/favorite-reading">
                    
                    阅读
                  </a>
                
                  
                  
                  
                  <a class="dropdown-item" href="/favorite-language/">
                    
                    语言
                  </a>
                
              </div>
            </li>
          
        
        
          <li class="nav-item" id="search-btn">
            <a class="nav-link" target="_self" href="javascript:;" data-toggle="modal" data-target="#modalSearch" aria-label="Search">
              &nbsp;<i class="iconfont icon-search"></i>&nbsp;
            </a>
          </li>
        
        
          <li class="nav-item" id="color-toggle-btn">
            <a class="nav-link" target="_self" href="javascript:;" aria-label="Color Toggle">&nbsp;<i
                class="iconfont icon-dark" id="color-toggle-icon"></i>&nbsp;</a>
          </li>
        
      </ul>
    </div>
  </div>
</nav>

    <div class="banner" id="banner" parallax=true
         style="background: url('/img/bg-wenzhang-2.jpg') no-repeat center center;
           background-size: cover;">
      <div class="full-bg-img">
        <div class="mask flex-center" style="background-color: rgba(0, 0, 0, 0.3)">
          <div class="page-header text-center fade-in-up">
            <span class="h2" id="subtitle" title="每天任务记录">
              
            </span>

            
              <div class="mt-3">
  
  
    <span class="post-meta">
      <i class="iconfont icon-date-fill" aria-hidden="true"></i>
      <time datetime="2021-11-15 11:19" pubdate>
        2021年11月15日 上午
      </time>
    </span>
  
</div>

<div class="mt-1">
  
    <span class="post-meta mr-2">
      <i class="iconfont icon-chart"></i>
      13k 字
    </span>
  

  
    <span class="post-meta mr-2">
      <i class="iconfont icon-clock-fill"></i>
      
      
      40 分钟
    </span>
  

  
  
    
      <!-- 不蒜子统计文章PV -->
      <span id="busuanzi_container_page_pv" style="display: none">
        <i class="iconfont icon-eye" aria-hidden="true"></i>
        <span id="busuanzi_value_page_pv"></span> 次
      </span>
    
  
</div>

            
          </div>

          
        </div>
      </div>
    </div>
  </header>

  <main>
    
      

<div class="container-fluid nopadding-x">
  <div class="row nomargin-x">
    <div class="d-none d-lg-block col-lg-2"></div>
    <div class="col-lg-8 nopadding-x-md">
      <div class="container nopadding-x-md" id="board-ctn">
        <div class="py-5" id="board">
          <article class="post-content mx-auto">
            <!-- SEO header -->
            <h1 style="display: none">每天任务记录</h1>
            
            <div class="markdown-body">
              <p>11-15<br>记得之后验证一下multiple是否支持只有loss这种情况，如果支持的话，sampler全部修改为一个，同时目前只修改和看BS这种情况</p>
<p>tune参数包括warm_up_epoch and sampling rate</p>
<p>完成情况：整理完之前的IOAS代码，将multi和single进行融合，同时使用字典形式来替换if-else多个flag的情况<br>记录：leetcode 刷题指南完成：贪心算法， 哈希表，双指针部分, 栈与队列部分</p>
<p>11-16<br>初步完成论文框架思维导图</p>
<p>修改代码，找到IO瓶颈场景</p>
<p>在代码中添加上视频分析这个场景</p>
<p>今天突然想到suprevision是用的什么anchor box，需要将数据下载完成后验证，猜测使用featmap_size??而不是预设的anchor</p>
<p>总结：<br>看完interviewtop上操作系统的八股文<br>下载coco数据<br>完成一版xmind思路调整<br>查看阿里基础架构面经</p>
<p>11-17<br>找到io为瓶颈的场景，并进行重要性采样实验<br>目前发现MS比BS精度高</p>
<p>BS：面临的问题  如何选择阈值 目前：搜索出初始thres_ratio, 之后使用历史+当前thres_ratio调整，//没有被计算的使用远程存储的模型进行计算,以及根据模型能力调整thres_ratio</p>
<p>MS: recall interval,不同等级之间的recall 时间差距是1 epoch 吗，新划分出来训练的类又如何根据阈值划分呢（等分感觉不科学）//建一个模型自动划分和确定等待的时间  （多个阈值和多个等待时间的确定）</p>
<p>现在只修改BS (1)实现在存储端计算样本重要性， （2）感觉样本的位置</p>
<p><a target="_blank" rel="noopener" href="https://www.sigarch.org/the-new-bottlenecks-of-ml-training-a-storage-perspective/">https://www.sigarch.org/the-new-bottlenecks-of-ml-training-a-storage-perspective/</a> 这个讲得挺好<br>处理的浏览了一下FreqCheck 的源码 但是每太明白细节上是如何在torch.save()后面封装 fsync(),   以及如何根据runtime overhead的profile结果进行自适应的checkpointing frequency超参数的调节</p>
<p>目标：弄懂多进程异步/模型的异步训练， 参考autoassist 以及checkFreq，subprocess<br>以及另外开一个进程来进行io带宽的测量(profile)以及未读取数据的重要性更新，另外一个空闲节点</p>
<p>11-18</p>
<p>看了autoassist的多进程，但是他们在训练另外一个模型进行样本的现在是训练了另外一个二分模型，这个二分模型的目标是 loss大于均值为target或者target model判断错误的label=1</p>
<p>我要训练的另外一个模型是在存储节点训练的模型，无需训练只需要将模型每次最新权重穿过来，在checkpoint的时候将模型传给远端; screen 需要同时训练另外一个模型会加重CPU和GPU训练负担，同时也不是很好同步轻量级模型和远程target模型判断样本重要性的差距</p>
<p>完成另外一个进程异步训练模型的初步代码</p>
<p>11-22</p>
<p>完成情况：<br>完成主进程和子进程相互传输重要性分数，invalid list以及模型初步代码</p>
<p>明天需要解决的问题：<br>1）如果解决模型参数保存的时候使用model.cpu().state_dict() 但是计算的时候需要使用dp 或者cuda, 保存模型这个动作如何转为复制模型参数，而不是真正将模型保存到cpu</p>
<ol start="2">
<li>如何减少推理时间太长的问题，如果使用原模型进行不重要样本重要性计算</li>
</ol>
<p>3）如果将一个节点转移到两个节点间的数据传输（远端存储系统情况）</p>
<p>4）把这个解决了，再解决如何修改io module 自定义一个缓冲区，然后在缓冲区/cache中实现TLB表，以及数据从缓冲区中读取</p>
<p>要解决的问题：1 分布式训练如何将另外一个进程计算得到的uni_score广播给其他进程<br>而且注意是多个分布式训练主进程对应一个子进程，不应该建立多个进行不重要样本重要性计算的子进程</p>
<p>目前遇到的问题：RuntimeError: module must have its parameters and buffers on device cuda:0 (device_ids[0]) but found one of them on device: cpu–&gt;解决，因为model.cpu().state_dict导致</p>
<p>11-23</p>
<p>完成情况：</p>
<p>1）使用一个copy.deepcopy完成在gpu上模型的拷贝到cpu<br>2) 解决子进程无法正常退出的问题<br>3）初步熟悉dataloader和dataset的代码</p>
<p>下一步：<br>如何解决判断数据是在缓存， 本地磁盘， 还是远端存储系统呢</p>
<p>11-24  </p>
<p>1）调整了lru_cache的逻辑<br>2）但是目前不知道如何将tlb得到的信息转为io_predict_time<br>3）初步使用多进程访问同一个cache,同时记录总访问次数和命中率，因为多进程需要共享一个数据结构，因此使用multiprocessing.Manager 因为LRUcache是自定义的一个数据结构，因此需要使用自定义的manager</p>
<p>目前发现worker越多，access 缓存次数越多</p>
<p>worker = 8<br>self.size:256<br>total_access_time:2432, hit_time:128</p>
<p>worker = 4<br>self.size:256<br>total_access_time:1048, hit_time:128</p>
<p>worker = 0<br>self.size:128<br>total_access_time:384, hit_time:256</p>
<p>如何只是对cache的每个变量使用multiprocessing进行加锁的话会导致出现None 结构remove错误，所以对于cache的get set函数都应该是一个原子操作（一个事务），所以需要对cache使用原子操作（加锁），而不仅仅是里面的一个变量加锁就可以了</p>
<p>11-25</p>
<p>验证多进程为什么总的访问次数会增加，以及为什么使用缓存的大小增加，但是命中率会减小<br>考虑如何将tlb功能转为IO score的计算<br>如何感知数据的位置，在其他节点的存储中，本地存储中，还是缓存中</p>
<p>发现：epoch=0和epoch=1的时候多进程取数据hit time=0， cache size=256 但是如果worker=0的话， epoch=1时候的命中率应该为128 cache size=128<br>解释：因为我在执行流程中间break了，但是数据还在提前取后面一轮的，所以导致cache size会满，同时命中率下降；第二轮本来要取的序列就减少了，所以多进程也没有讲缓存的数据冲走；也可以解释第0轮epoch就有2000多次的访问量</p>
<p>加上缓存：<br>Training done in 0.069387 hours<br>Training  done in 0.068791 hours</p>
<p>不加缓存：<br>Training  done in 0.017339 hours<br>Training  done in 0.035785 hours<br>Training  done in 0.037956 hours 命中率384</p>
<p>使用字典进行缓存，随机置换：<br>Training  done in 0.022509 hours<br>Training  done in 0.039637 hours  命中率 224</p>
<p>pyshmht 为python设计的基于hash表的共享内存</p>
<p>asyncio</p>
<p>目前结论：使用一个随机置换策略的缓存相比LRU置换效果更好  或者直接使用coodl的缓存后就固定不变</p>
<p>感觉应该是LRU每次加锁读写链表</p>
<p>明日安排：完成整个训练过程加缓存以及如何将缓存的位置转化为IO score</p>
<p>11-29</p>
<p>RPC小例子<br><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/136372142">https://zhuanlan.zhihu.com/p/136372142</a></p>
<p>了解一下rpc pytorch， 打算使用nfs再使用orangefs来实现两个节点上worker的通信<br>看了两篇论文：1.是分析深度学习不同存储方式的性能 2.运用机器学习判断应用的IO模式，然后自适应的选择相应的优化策略（主要是针对有读写，以及I/O模式在整个应用生命过程中会改变，但深度学习是只读的，I/O模式似乎都是一样的）<br><a target="_blank" rel="noopener" href="https://github.com/apachecn/apachecn-dl-zh/blob/master/docs/pt-tut-17/65.md">https://github.com/apachecn/apachecn-dl-zh/blob/master/docs/pt-tut-17/65.md</a></p>
<p>11-30<br>使用sudo aptitude install nfs-common 选择n y的时候安装失败，导致sudo无法使用，同时机器无法连接，修机器<br>参加rebuttal的会议</p>
<p>12-1</p>
<p>问题如何在orangefs上挂imagenet, 挂了之后仍然像ext4文件一样访问 /mnt/orangefs吗， 记得修改默认orangefs 服务器的文件目录，目前默认是/local500/storage-orangefs/吗<br>最后为什么是mount hec09的orangefs 为什么不是local500/storage-orangefs</p>
<p>12-2 </p>
<ol>
<li>完成 rebuttal的提交</li>
<li>初步在节点hec01 和hec03上实现 rpc测试  学长使用的是不同语言之间相互通信的grpc 先写一个.proto 文件，然后使用一个工具 将他编译为目标py文件或者go文件，然后分别在py和go文件里面使用这个编译好的文件</li>
</ol>
<p>在提交英文版本的时候一定要注意英语的拼写问题</p>
<p>pytorch官网上的rpc网络： <a target="_blank" rel="noopener" href="https://github.com/pytorch/pytorch/blob/master/docs/source/rpc.rst">https://github.com/pytorch/pytorch/blob/master/docs/source/rpc.rst</a></p>
<p>12-3 现在的问题是：worker如何确定当前可以shutdown</p>
<p>目前遇到的问题是如果是将远程过程调用放在sampler，如果是多进程取sampler的话会导致 出错，因此应该把远程过程调用的逻辑放在主函数里面</p>
<p>多进程会有多个sampler副本吗  难道是多进程访问sampler会加锁？？</p>
<p>明天 1.实现在每个epoch前进行重要性计算 train_one_epoch前  2. score通过字典传递，而不是list</p>
<p>rpc在远程过程调用的时候，如果调用的函数和初始化不在main函数下面py文件(主要的train_main 文件里)内会导致调用函数不一样 本来打算调用worker, 调用到master端</p>
<p>12-7<br>实现使用nfs， 以及torch.distributed.rpc实现两个任务之间的通信， 阅读grpc 的route_guide example源码， 如果要使用grpc需要使用异步+call back函数，或者使用一个子进程+同步grpc，然后实现主进程和子进程之间通过共享内存实现数据的共享</p>
<p>12-13</p>
<p>在hec01使用setattr修改了文件dataset的属性<br>现在的问题是存储单元如何得知自己存储节点的数据，或者假设存储节点非常的近，那么存储节点只需要派一个节点进行训练就好了</p>
<p>现在可以实现在一个存储单元上挂载orangefs-client查看数据， 之后换到新机器上实验orangefs形式的存储单元和计算单元 通讯和传递消息，目前server端只需要一个节点来进行推理，不需要每个server对每个server端的数据进行推理</p>
<p>12-14<br>找到一个bug， loss.mean().backward应该在loss.cpu.detach()前面，否则就会训练的时候精度一直为0，怀疑是torch版本的问题<br>打算再新环境中使用orangefs，然后进行通讯任务训练<br>以及考虑加上cache + IO score 预先训练cache中的数据</p>
<p>今天leetcode练习到532题</p>
<p>明日安排：</p>
<ol>
<li><p>先profile一下cache time 是否真的的比ssd time小很多 </p>
</li>
<li><p>同时查看一下是不是每轮epoch 第一个iteration时间最长， 找到网上解答原因</p>
</li>
<li><p>multi-criterion到底应该怎么用。</p>
</li>
<li><p>写一个简单的用cache的信息加入到multi-criterion计算（不同的优先级给定一个分数），以及在cache中采样第一个batch, 以及随机置换的cache策略，先写一个版本</p>
</li>
<li><p>cache size以及cache的转换策略将对io score产生怎样的影响</p>
</li>
</ol>
<p>相同数据每次的load time是不是都是变化很大。我感觉应该用时间近似，而不应该用level,因为不同cache的ssd的远近可能导致这两个时间相差很大，有的ssd读取和cache时间差不多那么io score应该就差不多，否则就应该加大io score的差距</p>
<p>感觉多进程预取会抵消cache好处？？如果IO是瓶颈的话，那么预取的效果就比从cache中取数据的效果差</p>
<p>12-15 完成一个简易版本的cache-aware sampling</p>
<p>主要包含下面三个功能的实现：</p>
<ol>
<li>IO score的计算问题 使用profile的cache的时间和ssd时间近似替代训练序列的all_result_list[‘iotime’]</li>
<li>sampler的序列问题， 先使用cache 里面缓存的序列</li>
<li>cache的置换问题，初步使用self.vis字典实现先替换cache中vis的序列，但是问题是每个epoch都需要clear一下cache的vis序列</li>
</ol>
<p>但是现在还是在注释中</p>
<p>12-16<br>尝试在8a100上实验，可能会考虑到分布式训练的sampler的修改<br>使之适应于分布式+orangefs这种场景</p>
<p>完成情况：目前在多GPU上调试了之前hec上面的代码，可以跑通单机多卡加上cache, 和修改了cache time计算方式的代码<br>初步完成昨天1， 2， 3个功能，目前的问题就是DataParallel+ another model in storage side可能会有问题</p>
<p>发现4 worker的时候 每4个iteration 第一个时间最长， 之后的时间都递减</p>
<p>明日计划：</p>
<p>四叉树合集有时间看看</p>
<p>12-17 数据传输慢是因为传输的带宽被占满了吗？ 是否可以开不同的数据取线程？来为用于级别cache增加命中率<br>为什么4worker时每个iteration 4个中必有一个时间长 （已解决）<br>这个iteration time和worker的设置有关</p>
<p>12-20<br>TODO: 分布式情况，多个节点得有多个cache, 这样的话在进行取数据的时候首先取cache中缓存的数据就不对，需要通过RDMA 需要直接取其他节点的cache数据才行<br>目前分析原因是消耗比读取快，消耗了一轮回过来，还是没有讲一个batch读完，所以又会在worker1处阻塞住，同时在worker1阻塞的时候，其他worker也在读取，所以表现为每隔一段时间就会出现一个IO时间非常长</p>
<p>但是如果计算是瓶颈，应该不会出现这个<br>resnet152的时候io_time就能够被计算时间覆盖了，所以所有的iotime大概都是0.0003左右</p>
<p>那么可以找到一个比较便捷的io瓶颈测量是将，看是否iotime是否随着worker进行周期变化； 但是总体上来说都是第一个iteration的iotime时间最长为1.6左右，因为没有任何计算来对iotime进行覆盖<br>alexnet其他worker每隔一段时间大概为0.5；resnet大概为0.2；其余为0.0003</p>
<p><a target="_blank" rel="noopener" href="https://kangsheng.xyz/2020/07/14/pytorch%E4%B8%ADdataloader%E7%9A%84num_workers%E8%AE%BE%E7%BD%AE/">https://kangsheng.xyz/2020/07/14/pytorch%E4%B8%ADdataloader%E7%9A%84num_workers%E8%AE%BE%E7%BD%AE/</a><br>这里的iotime包括进程开销，以及batch读取+batch处理开销 但是多进程加载数据包括读+处理两个部分， 而不是只读<br>8A100 2个物理CPU, 每个cpu 物理64核， 每个核有两个逻辑处理器；所以两个cpu一起就有256个逻辑处理器（2<em>2</em>64）<br>hec02是8个cpu, 每个cpu上面只有一个核</p>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/91521705">https://zhuanlan.zhihu.com/p/91521705</a> 查看dataset多worker加载数据，每个worker负责一个batch,只有在当前batch消耗掉之后才会取下一个分配的任务，而不是一直取数据。同时这个worker取数据包括读+数据增强两个部分</p>
<blockquote>
<p>其实各个worker之间读取数据时间差不多，并且由于是多进程任务，所以每次第一个worker读完数据就绪以后其他也准备就绪了，主进程随即开始连续前向传递了，并轮询的向各worker发送读新一批数据的信号，只不过跑完上一轮次所有worker产生的数据（num_workers个batch）后，各worker中新一轮次数据还没就绪，所以阻塞住了。</p>
</blockquote>
<p>4worker的情况下；<br>io_time: 0.6680457592010498<br>io_time: 0.00039649009704589844<br>io_time: 0.0004131793975830078<br>io_time: 0.00040602684020996094<br>io_time: 0.7091846466064453</p>
<p>8worker的情况下：<br>io_time: 0.583601713180542<br>io_time: 0.00048732757568359375<br>io_time: 0.011454343795776367<br>io_time: 0.0005695819854736328<br>io_time: 0.00035643577575683594<br>io_time: 0.0003287792205810547<br>io_time: 0.0004932880401611328<br>io_time: 0.0003211498260498047<br>io_time: 0.5443663597106934</p>
<p>16worker的时候：<br>io_time: 0.6202142238616943<br>io_time: 0.0005917549133300781<br>io_time: 0.0004055500030517578<br>io_time: 0.00042128562927246094<br>io_time: 0.0004863739013671875<br>io_time: 0.00027370452880859375<br>io_time: 0.0002474784851074219<br>io_time: 0.0002865791320800781<br>io_time: 0.00033855438232421875<br>io_time: 0.00021886825561523438<br>io_time: 0.0002658367156982422<br>io_time: 0.00020456314086914062<br>io_time: 0.0001895427703857422<br>io_time: 0.0005667209625244141<br>io_time: 0.0004878044128417969<br>io_time: 0.013859033584594727</p>
<p>worker数量越小，出现io_time突然增加的可能性越高<br>现在的问题是：在worker数量不足的时候是否这种用另外的worker去取数据，是否会影响数据加载的性能；相当于增加了线程数量；这好像更多加一个worker，然后多加一个缓存空间，实现超前预取没有实质区别<br>另外一种合理的解释就是说利用data worker取数据的空闲时间来进行cache的换新，但是IO 瓶颈data worker空闲的时间也不多呀；只有计算密集的data worker才会空闲</p>
<p>而且cache如果这样设计的话和buffer没有什么区别，因为都是取一次用一次，结论在buffer里面实现预取进程无用! 而且可能增加磁盘带宽压力</p>
<p>coordl又讲使用cache的理由吗？感觉可以直接沿用coordl理由</p>
<p>Gist：在两次数据利用之间有长时间的gap，因此可以对数据进行压缩，之后要用的时候再解压缩， ==在cache里面对数据进行压缩解压== 问题是data没有稀疏性，如果较好的无损压缩</p>
<p>12-21<br>目前分别在hec 和 8a100上测试远程过程调用和cache的代码</p>
<p>看代码的时候注意一下几个方面：<br>1）远程过程调用现在不能使用dataparallel  create_file<br>2) cache代码里面可以使用DP, 需要确定cache大小<br>3）loss.backward一定要在loss.cpu/记录loss 之前，否则会报错<br>4）合起来的时候一定要注意将init_rpc 以及 有关worker和master的代码打开（train_evaluate）<br>5) 感觉增加worker数量就能消除IO瓶颈的问题如何解决</p>
<p>找到一个bug: 使用dp的时候，batch_size应该*gpu数量，因为是一个batch size拆分，但是ddp不是</p>
<p>明天任务：<br>如何实现多GPU情况下，多进程共享一个数据结构cache， 因为现在好像是多个进程，每个可能都会创建一个cache这个是不合理的</p>
<p>12.22 </p>
<p>看了以及整理了一下括号相关的leetcode题目</p>
<p>rpc在回调结果的时候遇到一个问题：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><code class="hljs python">master使用远程过程调用的结果来更新criterion_history_list_dict<br>Traceback (most recent call last):<br>  File <span class="hljs-string">&quot;./imagenet_final_train_master.py&quot;</span>, line <span class="hljs-number">206</span>, <span class="hljs-keyword">in</span> &lt;module&gt;<br>    train_main(args.gpu, ngpus_per_node, args, kwargs)<br>  File <span class="hljs-string">&quot;./imagenet_final_train_master.py&quot;</span>, line <span class="hljs-number">94</span>, <span class="hljs-keyword">in</span> train_main<br>    run_training(model, trainloader, testloader, trainset, testset,  optimizer, criterion, dirs, args.epochs, args, kwargs)<br>  File <span class="hljs-string">&quot;/nfs/home/shu/hushuang_all_files/ImageClassification/8a100-code/IOAS/imagenet/train_evaluate.py&quot;</span>, line <span class="hljs-number">177</span>, <span class="hljs-keyword">in</span> run_training<br>    refresh_uni_list( uni_score_rref, allscore_numpy_dict, args)<br>  File <span class="hljs-string">&quot;/nfs/home/shu/hushuang_all_files/ImageClassification/8a100-code/IOAS/imagenet/rpc_func.py&quot;</span>, line <span class="hljs-number">44</span>, <span class="hljs-keyword">in</span> refresh_uni_list<br>    uni_score_list = uni_score_rref.to_here().numpy().tolist()<br>RuntimeError: RPCErr:<span class="hljs-number">1</span>:RPC ran <span class="hljs-keyword">for</span> more than <span class="hljs-built_in">set</span> timeout (<span class="hljs-number">60000</span> ms) <span class="hljs-keyword">and</span> will now be marked <span class="hljs-keyword">with</span> an error<br><br><span class="hljs-comment"># worker 端</span><br><br>[W tensorpipe_agent.cpp:<span class="hljs-number">498</span>] RPC agent <span class="hljs-keyword">for</span> worker1 encountered error when sending response to request <span class="hljs-comment">#16 to worker0: EOF: end of file</span><br></code></pre></td></tr></table></figure>


<p>12.23<br>现在8a100端注释了master的和worker代码就可以实现 存储端计算重要性+主进程端使用cache缓存部分数据</p>
<p>主要逻辑：（cache 3+ 存储单元 1）</p>
<ol>
<li>0epoch冷启动， 在1epoch缓存端已经开始缓存数据， 每轮训练的时候将vis的数据打一个标记，置换策略将vis的数据随机置换出去</li>
<li>下一轮 sampler返回训练的时候将cache中的数据放在前面</li>
<li>在重要性采样开始前一轮epoch，开始搜集重要性metric设计的分数，如果有io 分数的话，通过profile 的iotime 来对cache time的ssd load time进行估计（求平均值），同时对下一轮样本的iotime进行预测</li>
<li>将计算出来的重要的样本下一轮训练，不重要的样本序列通过rpc传给存储端的计算单元，让master和worker分别更新重要的样本和不重要样本的分数。</li>
<li>然后之后在进行采样的使用用全部更新好的重要性分数，重复排序和训练更新步骤</li>
</ol>
<p>解决两个bug:</p>
<ol>
<li>之所以8a100上面使用分布式调用不起来，是因为我之前在使用torch.distributed.rpc的时候写了os[‘environment’] =hec01  导致分布式错误</li>
<li>训练精度很低， 修改criterion<br>criterion = nn.CrossEntropyLoss(reduce=False).cuda(args.gpu) hec02实验<br>目前是用这个解决了，但是assert语句显示一致, 不知道原因</li>
</ol>
<p>12-27<br>解决cache在mp.spawn 使用多进程训练的时候导致段错误或者信号接收错误<br>选择启动方式：mp.set_start_method(‘spawn’)</p>
<p>在进行调试的时候，break不应该放在args,rank==0下面，否则就会卡死</p>
<p>12-28<br>现在的任务就是看rpc是否能和分布式训练的mp.spawn同时存在, 应该主进程主要负责rpc</p>
<p>ssh需要输入密码的节点为何无法使用rpc进行通信</p>
<p>12-29<br>今天主要看了legoos这篇论文， 对调度相关的其他论文也进行了总结和分类，以及看了ppt下面的链接有关内容<br>明日安排，做一下最短路径（单源或者多源情况）， 练习今天学到的有关Dijstra, Bellman-Ford算法以及Floyd算法</p>
<h4 id="2022"><a href="#2022" class="headerlink" title="2022"></a>2022</h4><p>1-3<br>完成&gt;400目标<br>代码主要就是看分布式+rpc是否能够兼容(+cache目前通过mp.spawn解决)并调试通过，实在ddp+rpc无法完成，就只能使用dp+rpc</p>
<p>1-6 现在遇到的问题：1.有ssh密码登录的机器，使用rpc无法正常运行， 同时rpc在同一台机器好像也无法正常运行</p>
<p>解决！<br>注意在rpc中发出rpc_async请求的是worker, 接收请求的是master</p>
<p>hec上使用pytorch 1.8.1只能在没有gpu的节点上，如果在hec01 hec02上就会报错</p>
<p>现在问题 8a100和hec06节点连接会报错：<br>aster: ECONNREFUSED: connection refused<br>Failed to respond to ‘Shutdown Proceed’ in time, got error ECONNREFUSED: connection refused<br>Traceback (most recent call last):<br>  File “worker.py”, line 88, in <module><br>    rpc.shutdown()<br>  File “/nfs/home/anaconda3/envs/pytorch-shu/lib/python3.6/site-packages/torch/distributed/rpc/api.py”, line 78, in wrapper<br>    return func(*args, **kwargs)<br>  File “/nfs/home/anaconda3/envs/pytorch-shu/lib/python3.6/site-packages/torch/distributed/rpc/api.py”, line 284, in shutdown<br>    _get_current_rpc_agent().join()<br>RuntimeError: [/opt/conda/conda-bld/pytorch_1607370193460/work/third_party/gloo/gloo/transport/tcp/pair.cc:575] Connection closed by peer [10.5.30.42]:27806</module></p>
<p>Solution: Edit /etc/hosts and change 127.0.1.1 to your actual ip-address 目前看到的一个答案<br>分析：现在在hec上互相使用rpc（即使有密码）是可以的，但是在8a100就不行，所以现在怀疑的是需要在host里面添加目录，以及需要在两台机器上换成同一个用户才行<br>第二种解决方案，应该使用tpc的init_method，以及使用gloo后端，但是目前试验过，仍然会报错</p>
<p>感觉在分布式多台机器上训练也会出现相同的问题</p>
<p>找到的真正的解决方案：backend=rpc.BackendType.PROCESS_GROUP， 不需要加其他的<br>os.environ[‘TF_SOCKET_IFNAME’] = ‘ens3’ (x)<br>os.environ[‘GLOO_SOCKET_IFNAME’] = ‘ens3’ (x)</p>
<p>寒假安排：</p>
<ol>
<li>leetcode争取上500 可以先将宫水三叶的题刷完，应该都是经典题目</li>
<li>对之前写的leetcode进行总结，整理，转为每个知识点，先放总结的思路，或者针对的题型，然后放相应的例题，没道例题简答写一下解决方案，和总结的思路进行对应</li>
<li>回归操作系统，计网的基础知识, 转为笔试的xmind (408题目看看)<br>github和牛客网应该有许多资源，多看看多比较; 包括深度学习和目标检测相关的面试题目<br>牛客网上， 网友的总结：(CyC2018)<br>[<a target="_blank" rel="noopener" href="https://github.com/CyC2018/CS-Notes/blob/master/notes/%E8%AE%A1%E7%AE%97%E6%9C%BA%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%20-%20%E7%9B%AE%E5%BD%95.md]">https://github.com/CyC2018/CS-Notes/blob/master/notes/%E8%AE%A1%E7%AE%97%E6%9C%BA%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%20-%20%E7%9B%AE%E5%BD%95.md]</a></li>
<li>补充面试的xmind</li>
<li>有时间学一下grpc</li>
<li>记得备份一下8a100上面的数据 (目前正在8a100使用rsync进行传输 到hec06 /nfs目录下)<br>7。一些公司名单和深度学习的资料<br><a target="_blank" rel="noopener" href="https://github.com/amusi/CV-Company-List">https://github.com/amusi/CV-Company-List</a><br>注意关注一下春招信息，微软已经开始投递了，然后我因为错过了问卷，错过了提前批，之后注意准备面试和之后的问卷</li>
<li>修改简历，变为项目两个， research两个，将重要性采样自己的工作写上去zzw</li>
<li>项目一定要非常非常熟悉，整理面试可能问到的每一个问题，和自己优化的思考！！！</li>
</ol>
<p>刷完八股之后，进行自测：<a target="_blank" rel="noopener" href="https://github.com/youngyangyang04/TechCPP">https://github.com/youngyangyang04/TechCPP</a><br><a target="_blank" rel="noopener" href="https://github.com/youngyangyang04/leetcode-master">https://github.com/youngyangyang04/leetcode-master</a></p>
<p>有关于C++的八股文：<a target="_blank" rel="noopener" href="https://github.com/Light-City/CPlusPlusThings">https://github.com/Light-City/CPlusPlusThings</a></p>
<p>408资料<br><a target="_blank" rel="noopener" href="https://github.com/CodePanda66/CSPostgraduate-408">https://github.com/CodePanda66/CSPostgraduate-408</a><br>408历年真题大题查看</p>
<p>公司：思科，</p>
<p>别人的面经总结：<br><a target="_blank" rel="noopener" href="https://prohuper.github.io/2020/04/17/interview_exp2/">https://prohuper.github.io/2020/04/17/interview_exp2/</a></p>
<p>推荐资料:<br>力扣1-300题（前300道题非常经典，建议学有余力的同学都刷一刷）<br>力扣HOT100（跟上面有不少是重复的，刷的时候要注意总结）<br>啊哈！算法、大话数据结构（这两本书都是面向新手的图书，图画很多）<br>剑指offer（这本书不需要多做介绍，校招必备）<br>挑战程序设计竞赛（这本书属于进阶一点的算法书籍了，作者是ACM-ICPC全球总冠军，可以说是<br>世界顶级程序设计高手的经验总结了，需要慢慢消化，经典题型太多）<br>程序员代码面试指南（左程云大神的书，我并没有看完，只是看了其中的海量数据处理部分的题<br>目就已经十分受用了，在某大厂三面中就考查到了其中的海量数据集处理的问题）</p>
<p>按公司分题：<a target="_blank" rel="noopener" href="https://github.com/afatcoder/LeetcodeTop%EF%BC%88%E9%9D%A2%E8%AF%95%E5%89%8D%E5%88%B7%E4%B8%80%E5%88%B7%EF%BC%89">https://github.com/afatcoder/LeetcodeTop（面试前刷一刷）</a></p>
<p><a target="_blank" rel="noopener" href="https://github.com/HarleysZhang/2021_algorithm_intern_information">https://github.com/HarleysZhang/2021_algorithm_intern_information</a> 计算机视觉-算法岗位相关<br><a target="_blank" rel="noopener" href="https://github.com/DWCTOD/interview">https://github.com/DWCTOD/interview</a></p>
<p>把leetcode的剑指offer开头 和leetcode全部看一篇</p>

            </div>
            <hr>
            <div>
              <div class="post-metas mb-3">
                
                  <div class="post-meta mr-3">
                    <i class="iconfont icon-category"></i>
                    
                      <a class="hover-with-bg" href="/categories/%E6%97%A5%E5%B8%B8%E8%AE%B0%E4%BA%8B/">日常记事</a>
                    
                  </div>
                
                
                  <div class="post-meta">
                    <i class="iconfont icon-tags"></i>
                    
                      <a class="hover-with-bg" href="/tags/daily-task/">daily task</a>
                    
                  </div>
                
              </div>
              
                <p class="note note-warning">
                  
                    本博客所有文章除特别声明外，均采用 <a target="_blank" href="https://creativecommons.org/licenses/by-sa/4.0/deed.zh" rel="nofollow noopener noopener">CC BY-SA 4.0 协议</a> ，转载请注明出处！
                  
                </p>
              
              
            </div>

            
              <!-- Comments -->
              <article class="comments" id="comments" lazyload>
                
                  
                
                

              </article>
            
          </article>
        </div>
      </div>
    </div>
    
      <div class="d-none d-lg-block col-lg-2 toc-container" id="toc-ctn">
        <div id="toc">
  <p class="toc-header"><i class="iconfont icon-list"></i>&nbsp;目录</p>
  <div class="toc-body" id="toc-body"></div>
</div>

      </div>
    
  </div>
</div>

<!-- Custom -->


    

    
      <a id="scroll-top-button" aria-label="TOP" href="#" role="button">
        <i class="iconfont icon-arrowup" aria-hidden="true"></i>
      </a>
    

    
      <div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel"
     aria-hidden="true">
  <div class="modal-dialog modal-dialog-scrollable modal-lg" role="document">
    <div class="modal-content">
      <div class="modal-header text-center">
        <h4 class="modal-title w-100 font-weight-bold">搜索</h4>
        <button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body mx-3">
        <div class="md-form mb-5">
          <input type="text" id="local-search-input" class="form-control validate">
          <label data-error="x" data-success="v"
                 for="local-search-input">关键词</label>
        </div>
        <div class="list-group" id="local-search-result"></div>
      </div>
    </div>
  </div>
</div>
    

    
  </main>

  <footer class="text-center mt-5 py-3">
  <div class="footer-content">
     <a href="https://hexo.io" target="_blank" rel="nofollow noopener"><span>Hexo</span></a> <i class="iconfont icon-love"></i> <a href="https://github.com/fluid-dev/hexo-theme-fluid" target="_blank" rel="nofollow noopener"><span>Fluid</span></a> 
  </div>
  

  

  
</footer>


  <!-- SCRIPTS -->
  
  <script  src="https://cdn.jsdelivr.net/npm/nprogress@0/nprogress.min.js" ></script>
  <link  rel="stylesheet" href="https://cdn.jsdelivr.net/npm/nprogress@0/nprogress.min.css" />

  <script>
    NProgress.configure({"showSpinner":false,"trickleSpeed":100})
    NProgress.start()
    window.addEventListener('load', function() {
      NProgress.done();
    })
  </script>


<script  src="https://cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js" ></script>
<script  src="https://cdn.jsdelivr.net/npm/bootstrap@4/dist/js/bootstrap.min.js" ></script>
<script  src="/js/events.js" ></script>
<script  src="/js/plugins.js" ></script>

<!-- Plugins -->


  <script  src="/js/local-search.js" ></script>



  
    <script  src="/js/img-lazyload.js" ></script>
  



  



  
    <script  src="https://cdn.jsdelivr.net/npm/tocbot@4/dist/tocbot.min.js" ></script>
  
  
    <script  src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3/dist/jquery.fancybox.min.js" ></script>
  
  
    <script  src="https://cdn.jsdelivr.net/npm/anchor-js@4/anchor.min.js" ></script>
  
  
    <script defer src="https://cdn.jsdelivr.net/npm/clipboard@2/dist/clipboard.min.js" ></script>
  



  <script defer src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" ></script>




  <script  src="https://cdn.jsdelivr.net/npm/typed.js@2/lib/typed.min.js" ></script>
  <script>
    (function (window, document) {
      var typing = Fluid.plugins.typing;
      var title = document.getElementById('subtitle').title;
      
      typing(title)
      
    })(window, document);
  </script>















<!-- 主题的启动项 保持在最底部 -->
<script  src="/js/boot.js" ></script>


</body>
</html>
